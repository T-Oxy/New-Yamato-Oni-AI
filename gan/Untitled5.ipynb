{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(0)\n",
    "np.random.RandomState(0)\n",
    "tf.compat.v1.set_random_seed(0)\n",
    "\n",
    "config = tf.compat.v1.ConfigProto(gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "#dcgan_v3.pyが保存されているディレクトリのフルパス\n",
    "root_dir = \"/Users/user/Desktop/m31_expt/m31_datasets/\"\n",
    "#参照する画像フォルダ\n",
    "input_img_dir = \"demon_resize\"\n",
    "#出力する画像フォルダ\n",
    "save_dir = \"dcgan_v3_demon_img/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.class_names = os.listdir(root_dir)\n",
    "\n",
    "        self.shape = (128, 128, 3)\n",
    "        self.z_dim = 100\n",
    "\n",
    "        optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        self.generator = self.build_generator()\n",
    "        # self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        z = Input(shape=(self.z_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        noise_shape = (self.z_dim,)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 32 * 32, activation=\"relu\", input_shape=noise_shape))\n",
    "        model.add(Reshape((32, 32, 128)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(3, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        img_shape = self.shape\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def build_combined(self):\n",
    "        self.discriminator.trainable = False\n",
    "        model = Sequential([self.generator, self.discriminator])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, iterations, batch_size=128, save_interval=50, model_interval=10000, check_noise=None, r=5, c=5):\n",
    "\n",
    "        X_train, labels = self.load_imgs()\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "\n",
    "            # ------------------\n",
    "            # Training Discriminator\n",
    "            # -----------------\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.uniform(-1, 1, (half_batch, self.z_dim))\n",
    "\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # -----------------\n",
    "            # Training Generator\n",
    "            # -----------------\n",
    "\n",
    "            noise = np.random.uniform(-1, 1, (batch_size, self.z_dim))\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (iteration, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "            if iteration % save_interval == 0:\n",
    "                self.save_imgs(iteration, check_noise, r, c)\n",
    "                start = np.expand_dims(check_noise[0], axis=0)\n",
    "                end = np.expand_dims(check_noise[1], axis=0)\n",
    "                resultImage = self.visualizeInterpolation(start=start, end=end)\n",
    "                cv2.imwrite(save_dir + \"latent_{}.png\".format(iteration), resultImage)\n",
    "                if iteration % model_interval == 0:\n",
    "                    self.generator.save(\"mb_dcgan-{}-iter.h5\".format(iteration))\n",
    "\n",
    "    def save_imgs(self, iteration, check_noise, r, c):\n",
    "        noise = check_noise\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # 0-1 rescale\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, :])\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(save_dir + '%d.png' % iteration)\n",
    "        # fig.savefig('images/gen_imgs/kill_me_%d.png' % iteration)\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def load_imgs(self):\n",
    "\n",
    "        img_paths = []\n",
    "        labels = []\n",
    "        images = []\n",
    "\n",
    "    #print(input_img_dir)\n",
    "    #print(self.class_names)\n",
    "\n",
    "        for cl_name in self.class_names:\n",
    "            if cl_name == input_img_dir:\n",
    "                img_names = os.listdir(os.path.join(root_dir, cl_name))\n",
    "\n",
    "\n",
    "\n",
    "                for img_name in img_names:\n",
    "                    img_paths.append(os.path.abspath(os.path.join(root_dir, cl_name, img_name)))\n",
    "                    hot_cl_name = self.get_class_one_hot(cl_name)\n",
    "                    labels.append(hot_cl_name)\n",
    "\n",
    "        for img_path in img_paths:\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            images.append(img)\n",
    "\n",
    "        images = np.array(images)\n",
    "\n",
    "        return (np.array(images), np.array(labels))\n",
    "\n",
    "    def get_class_one_hot(self, class_str):\n",
    "        label_encoded = self.class_names.index(class_str)\n",
    "\n",
    "        label_hot = np_utils.to_categorical(label_encoded, len(self.class_names))\n",
    "        label_hot = label_hot\n",
    "\n",
    "        return label_hot\n",
    "\n",
    "    def visualizeInterpolation(self, start, end, save=True, nbSteps=10):\n",
    "        print(\"Generating interpolations...\")\n",
    "\n",
    "        steps = nbSteps\n",
    "        latentStart = start\n",
    "        latentEnd = end\n",
    "\n",
    "        startImg = self.generator.predict(latentStart)\n",
    "        endImg = self.generator.predict(latentEnd)\n",
    "\n",
    "        vectors = []\n",
    "\n",
    "        alphaValues = np.linspace(0, 1, steps)\n",
    "        for alpha in alphaValues:\n",
    "            vector = latentStart * (1 - alpha) + latentEnd * alpha\n",
    "            vectors.append(vector)\n",
    "\n",
    "        vectors = np.array(vectors)\n",
    "\n",
    "        resultLatent = None\n",
    "        resultImage = None\n",
    "\n",
    "        for i, vec in enumerate(vectors):\n",
    "            gen_img = np.squeeze(self.generator.predict(vec), axis=0)\n",
    "            gen_img = (0.5 * gen_img + 0.5) * 255\n",
    "            interpolatedImage = cv2.cvtColor(gen_img, cv2.COLOR_RGB2BGR)\n",
    "            interpolatedImage = interpolatedImage.astype(np.uint8)\n",
    "            resultImage = interpolatedImage if resultImage is None else np.hstack([resultImage, interpolatedImage])\n",
    "\n",
    "        return resultImage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 33, 33, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 17, 17, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 17, 17, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 73984)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 73985     \n",
      "=================================================================\n",
      "Total params: 463,169\n",
      "Trainable params: 462,785\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 131072)            13238272  \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 128, 64)      73792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 128, 128, 3)       1731      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 128, 3)       0         \n",
      "=================================================================\n",
      "Total params: 13,462,659\n",
      "Trainable params: 13,462,019\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "0 [D loss: 1.843931, acc.: 12.50%] [G loss: 0.570140]\n",
      "Generating interpolations...\n",
      "1 [D loss: 1.383734, acc.: 50.00%] [G loss: 0.806236]\n",
      "2 [D loss: 0.956581, acc.: 46.88%] [G loss: 0.744658]\n",
      "3 [D loss: 1.008485, acc.: 31.25%] [G loss: 0.425931]\n",
      "4 [D loss: 0.454715, acc.: 90.62%] [G loss: 0.343383]\n",
      "5 [D loss: 0.220727, acc.: 100.00%] [G loss: 0.143847]\n",
      "6 [D loss: 0.254992, acc.: 93.75%] [G loss: 0.044912]\n",
      "7 [D loss: 0.139373, acc.: 100.00%] [G loss: 0.012769]\n",
      "8 [D loss: 0.096917, acc.: 100.00%] [G loss: 0.005029]\n",
      "9 [D loss: 0.074899, acc.: 100.00%] [G loss: 0.000716]\n",
      "10 [D loss: 0.076043, acc.: 100.00%] [G loss: 0.000584]\n",
      "11 [D loss: 0.066768, acc.: 100.00%] [G loss: 0.000138]\n",
      "12 [D loss: 0.042119, acc.: 100.00%] [G loss: 0.000107]\n",
      "13 [D loss: 0.045438, acc.: 100.00%] [G loss: 0.000058]\n",
      "14 [D loss: 0.039202, acc.: 100.00%] [G loss: 0.000088]\n",
      "15 [D loss: 0.023863, acc.: 100.00%] [G loss: 0.000052]\n",
      "16 [D loss: 0.048612, acc.: 100.00%] [G loss: 0.000023]\n",
      "17 [D loss: 0.033070, acc.: 100.00%] [G loss: 0.000045]\n",
      "18 [D loss: 0.037157, acc.: 100.00%] [G loss: 0.000099]\n",
      "19 [D loss: 0.030272, acc.: 100.00%] [G loss: 0.000565]\n",
      "20 [D loss: 0.057021, acc.: 100.00%] [G loss: 0.001947]\n",
      "21 [D loss: 0.027985, acc.: 100.00%] [G loss: 0.002298]\n",
      "22 [D loss: 0.133445, acc.: 93.75%] [G loss: 0.010740]\n",
      "23 [D loss: 0.145450, acc.: 93.75%] [G loss: 0.004180]\n",
      "24 [D loss: 0.063476, acc.: 100.00%] [G loss: 0.010308]\n",
      "25 [D loss: 0.097574, acc.: 96.88%] [G loss: 0.131328]\n",
      "26 [D loss: 0.044662, acc.: 100.00%] [G loss: 0.282232]\n",
      "27 [D loss: 0.955479, acc.: 62.50%] [G loss: 3.119730]\n",
      "28 [D loss: 0.509944, acc.: 71.88%] [G loss: 0.372569]\n",
      "29 [D loss: 0.423460, acc.: 84.38%] [G loss: 2.074071]\n",
      "30 [D loss: 0.104112, acc.: 96.88%] [G loss: 2.634576]\n",
      "31 [D loss: 0.229084, acc.: 90.62%] [G loss: 0.162591]\n",
      "32 [D loss: 0.242014, acc.: 93.75%] [G loss: 0.604318]\n",
      "33 [D loss: 0.034235, acc.: 100.00%] [G loss: 1.432888]\n",
      "34 [D loss: 0.042402, acc.: 100.00%] [G loss: 0.612642]\n",
      "35 [D loss: 0.048000, acc.: 100.00%] [G loss: 0.746419]\n",
      "36 [D loss: 0.042131, acc.: 100.00%] [G loss: 0.853574]\n",
      "37 [D loss: 0.050956, acc.: 96.88%] [G loss: 0.896813]\n",
      "38 [D loss: 0.019260, acc.: 100.00%] [G loss: 0.726093]\n",
      "39 [D loss: 0.013836, acc.: 100.00%] [G loss: 0.632048]\n",
      "40 [D loss: 0.016004, acc.: 100.00%] [G loss: 0.610088]\n",
      "41 [D loss: 0.013076, acc.: 100.00%] [G loss: 0.447029]\n",
      "42 [D loss: 0.190366, acc.: 96.88%] [G loss: 0.270244]\n",
      "43 [D loss: 0.007333, acc.: 100.00%] [G loss: 0.765481]\n",
      "44 [D loss: 0.013457, acc.: 100.00%] [G loss: 0.782894]\n",
      "45 [D loss: 0.004597, acc.: 100.00%] [G loss: 0.285658]\n",
      "46 [D loss: 0.003105, acc.: 100.00%] [G loss: 0.385784]\n",
      "47 [D loss: 0.006997, acc.: 100.00%] [G loss: 0.447862]\n",
      "48 [D loss: 0.010917, acc.: 100.00%] [G loss: 0.332373]\n",
      "49 [D loss: 0.003318, acc.: 100.00%] [G loss: 0.249254]\n",
      "50 [D loss: 0.098576, acc.: 100.00%] [G loss: 0.090656]\n",
      "Generating interpolations...\n",
      "51 [D loss: 0.005671, acc.: 100.00%] [G loss: 0.121614]\n",
      "52 [D loss: 0.004435, acc.: 100.00%] [G loss: 0.129799]\n",
      "53 [D loss: 0.001984, acc.: 100.00%] [G loss: 0.127102]\n",
      "54 [D loss: 0.002329, acc.: 100.00%] [G loss: 0.094668]\n",
      "55 [D loss: 0.001628, acc.: 100.00%] [G loss: 0.104824]\n",
      "56 [D loss: 0.004440, acc.: 100.00%] [G loss: 0.105366]\n",
      "57 [D loss: 0.000975, acc.: 100.00%] [G loss: 0.099672]\n",
      "58 [D loss: 0.001868, acc.: 100.00%] [G loss: 0.142157]\n",
      "59 [D loss: 0.002026, acc.: 100.00%] [G loss: 0.109746]\n",
      "60 [D loss: 0.001613, acc.: 100.00%] [G loss: 0.057178]\n",
      "61 [D loss: 0.002465, acc.: 100.00%] [G loss: 0.041515]\n",
      "62 [D loss: 0.002387, acc.: 100.00%] [G loss: 0.077419]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 [D loss: 0.002611, acc.: 100.00%] [G loss: 0.130689]\n",
      "64 [D loss: 0.000804, acc.: 100.00%] [G loss: 0.087328]\n",
      "65 [D loss: 0.000546, acc.: 100.00%] [G loss: 0.122628]\n",
      "66 [D loss: 0.001092, acc.: 100.00%] [G loss: 0.057421]\n",
      "67 [D loss: 0.001878, acc.: 100.00%] [G loss: 0.042023]\n",
      "68 [D loss: 0.000853, acc.: 100.00%] [G loss: 0.057980]\n",
      "69 [D loss: 0.001015, acc.: 100.00%] [G loss: 0.093085]\n",
      "70 [D loss: 0.002179, acc.: 100.00%] [G loss: 0.073897]\n",
      "71 [D loss: 0.001202, acc.: 100.00%] [G loss: 0.077942]\n",
      "72 [D loss: 0.001372, acc.: 100.00%] [G loss: 0.095400]\n",
      "73 [D loss: 0.001127, acc.: 100.00%] [G loss: 0.069875]\n",
      "74 [D loss: 0.002175, acc.: 100.00%] [G loss: 0.107098]\n",
      "75 [D loss: 0.001070, acc.: 100.00%] [G loss: 0.048146]\n",
      "76 [D loss: 0.001700, acc.: 100.00%] [G loss: 0.084303]\n",
      "77 [D loss: 0.001344, acc.: 100.00%] [G loss: 0.101491]\n",
      "78 [D loss: 0.001271, acc.: 100.00%] [G loss: 0.072396]\n",
      "79 [D loss: 0.000741, acc.: 100.00%] [G loss: 0.071097]\n",
      "80 [D loss: 0.017712, acc.: 100.00%] [G loss: 0.093052]\n",
      "81 [D loss: 0.001310, acc.: 100.00%] [G loss: 0.112007]\n",
      "82 [D loss: 0.001598, acc.: 100.00%] [G loss: 0.100527]\n",
      "83 [D loss: 0.001186, acc.: 100.00%] [G loss: 0.049218]\n",
      "84 [D loss: 0.000521, acc.: 100.00%] [G loss: 0.048435]\n",
      "85 [D loss: 0.001730, acc.: 100.00%] [G loss: 0.124533]\n",
      "86 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.066588]\n",
      "87 [D loss: 0.002850, acc.: 100.00%] [G loss: 0.082976]\n",
      "88 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.052650]\n",
      "89 [D loss: 0.001019, acc.: 100.00%] [G loss: 0.064422]\n",
      "90 [D loss: 0.000712, acc.: 100.00%] [G loss: 0.069360]\n",
      "91 [D loss: 0.006043, acc.: 100.00%] [G loss: 0.112193]\n",
      "92 [D loss: 0.002552, acc.: 100.00%] [G loss: 0.075456]\n",
      "93 [D loss: 0.000500, acc.: 100.00%] [G loss: 0.075077]\n",
      "94 [D loss: 0.000680, acc.: 100.00%] [G loss: 0.028509]\n",
      "95 [D loss: 0.001308, acc.: 100.00%] [G loss: 0.031943]\n",
      "96 [D loss: 0.000661, acc.: 100.00%] [G loss: 0.029469]\n",
      "97 [D loss: 0.000767, acc.: 100.00%] [G loss: 0.030371]\n",
      "98 [D loss: 0.001638, acc.: 100.00%] [G loss: 0.040169]\n",
      "99 [D loss: 0.000958, acc.: 100.00%] [G loss: 0.037931]\n",
      "100 [D loss: 0.001054, acc.: 100.00%] [G loss: 0.089514]\n",
      "Generating interpolations...\n",
      "101 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.056267]\n",
      "102 [D loss: 0.001044, acc.: 100.00%] [G loss: 0.047081]\n",
      "103 [D loss: 0.001015, acc.: 100.00%] [G loss: 0.052657]\n",
      "104 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.028716]\n",
      "105 [D loss: 0.001772, acc.: 100.00%] [G loss: 0.051524]\n",
      "106 [D loss: 0.000670, acc.: 100.00%] [G loss: 0.046691]\n",
      "107 [D loss: 0.000680, acc.: 100.00%] [G loss: 0.060862]\n",
      "108 [D loss: 0.000531, acc.: 100.00%] [G loss: 0.029326]\n",
      "109 [D loss: 0.001725, acc.: 100.00%] [G loss: 0.030167]\n",
      "110 [D loss: 0.000365, acc.: 100.00%] [G loss: 0.080563]\n",
      "111 [D loss: 0.011165, acc.: 100.00%] [G loss: 0.035634]\n",
      "112 [D loss: 0.000677, acc.: 100.00%] [G loss: 0.016367]\n",
      "113 [D loss: 0.000426, acc.: 100.00%] [G loss: 0.013967]\n",
      "114 [D loss: 0.000735, acc.: 100.00%] [G loss: 0.045551]\n",
      "115 [D loss: 0.000823, acc.: 100.00%] [G loss: 0.019979]\n",
      "116 [D loss: 0.000469, acc.: 100.00%] [G loss: 0.021628]\n",
      "117 [D loss: 0.000456, acc.: 100.00%] [G loss: 0.016109]\n",
      "118 [D loss: 0.000947, acc.: 100.00%] [G loss: 0.015665]\n",
      "119 [D loss: 0.000790, acc.: 100.00%] [G loss: 0.011636]\n",
      "120 [D loss: 0.000918, acc.: 100.00%] [G loss: 0.010387]\n",
      "121 [D loss: 0.000379, acc.: 100.00%] [G loss: 0.012588]\n",
      "122 [D loss: 0.000407, acc.: 100.00%] [G loss: 0.013711]\n",
      "123 [D loss: 0.002066, acc.: 100.00%] [G loss: 0.019165]\n",
      "124 [D loss: 0.000248, acc.: 100.00%] [G loss: 0.015309]\n",
      "125 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.015209]\n",
      "126 [D loss: 0.000421, acc.: 100.00%] [G loss: 0.018966]\n",
      "127 [D loss: 0.000316, acc.: 100.00%] [G loss: 0.018220]\n",
      "128 [D loss: 0.000470, acc.: 100.00%] [G loss: 0.013479]\n",
      "129 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.023918]\n",
      "130 [D loss: 0.000287, acc.: 100.00%] [G loss: 0.029879]\n",
      "131 [D loss: 0.000732, acc.: 100.00%] [G loss: 0.034649]\n",
      "132 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.010333]\n",
      "133 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.026352]\n",
      "134 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.023510]\n",
      "135 [D loss: 0.000340, acc.: 100.00%] [G loss: 0.025483]\n",
      "136 [D loss: 0.000116, acc.: 100.00%] [G loss: 0.018274]\n",
      "137 [D loss: 0.000515, acc.: 100.00%] [G loss: 0.036526]\n",
      "138 [D loss: 0.000526, acc.: 100.00%] [G loss: 0.028825]\n",
      "139 [D loss: 0.000374, acc.: 100.00%] [G loss: 0.023119]\n",
      "140 [D loss: 0.000249, acc.: 100.00%] [G loss: 0.026624]\n",
      "141 [D loss: 0.000443, acc.: 100.00%] [G loss: 0.021491]\n",
      "142 [D loss: 0.000332, acc.: 100.00%] [G loss: 0.017205]\n",
      "143 [D loss: 0.000118, acc.: 100.00%] [G loss: 0.027951]\n",
      "144 [D loss: 0.000396, acc.: 100.00%] [G loss: 0.024482]\n",
      "145 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.014593]\n",
      "146 [D loss: 0.000148, acc.: 100.00%] [G loss: 0.011449]\n",
      "147 [D loss: 0.000240, acc.: 100.00%] [G loss: 0.012799]\n",
      "148 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.011474]\n",
      "149 [D loss: 0.000411, acc.: 100.00%] [G loss: 0.010682]\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    r, c = 5, 5\n",
    "    check_noise = np.random.uniform(-1, 1, (r * c, 100))\n",
    "    dcgan.train(\n",
    "        iterations=200000,\n",
    "        batch_size=32,\n",
    "        # save_interval=1000,\n",
    "        save_interval=50, ### epoch回数が50の倍数になったときに、generator生成画像を保存\n",
    "        model_interval=5000,\n",
    "        check_noise=check_noise,\n",
    "        r=r,\n",
    "        c=c\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
