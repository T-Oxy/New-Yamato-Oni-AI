{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Input, Dense, Reshape, Flatten, Dropout\n",
    "from keras.layers import BatchNormalization, Activation, ZeroPadding2D\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras.layers.convolutional import UpSampling2D, Conv2D\n",
    "from keras.models import Sequential, Model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import np_utils\n",
    "import tensorflow as tf\n",
    "from tensorflow.compat.v1.keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = tf.compat.v1.ConfigProto(gpu_options=tf.compat.v1.GPUOptions(allow_growth=True))\n",
    "session = tf.compat.v1.Session(config=config)\n",
    "K.set_session(session)\n",
    "\n",
    "#dcgan_v3.pyが保存されているディレクトリのフルパス\n",
    "root_dir = \"/Users/user/Desktop/m31_expt/m31_datasets/\"\n",
    "#参照する画像フォルダ\n",
    "input_img_dir = \"human_w1_resize\"\n",
    "#出力する画像フォルダ\n",
    "save_dir = \"dcgan_v3_human_w1_img/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DCGAN():\n",
    "    def __init__(self):\n",
    "\n",
    "        self.class_names = os.listdir(root_dir)\n",
    "\n",
    "        self.shape = (128, 128, 3)\n",
    "        self.z_dim = 100\n",
    "\n",
    "        optimizer = Adam(lr=0.0002, beta_1=0.5)\n",
    "\n",
    "        self.discriminator = self.build_discriminator()\n",
    "        self.discriminator.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        self.generator = self.build_generator()\n",
    "        # self.generator.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "        z = Input(shape=(self.z_dim,))\n",
    "        img = self.generator(z)\n",
    "\n",
    "        self.discriminator.trainable = False\n",
    "\n",
    "        valid = self.discriminator(img)\n",
    "\n",
    "        self.combined = Model(z, valid)\n",
    "        self.combined.compile(loss='binary_crossentropy', optimizer=optimizer)\n",
    "\n",
    "    def build_generator(self):\n",
    "        noise_shape = (self.z_dim,)\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Dense(128 * 32 * 32, activation=\"relu\", input_shape=noise_shape))\n",
    "        model.add(Reshape((32, 32, 128)))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(UpSampling2D())\n",
    "        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"relu\"))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(3, kernel_size=3, padding=\"same\"))\n",
    "        model.add(Activation(\"tanh\"))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        noise = Input(shape=noise_shape)\n",
    "        img = model(noise)\n",
    "\n",
    "        return Model(noise, img)\n",
    "\n",
    "    def build_discriminator(self):\n",
    "        img_shape = self.shape\n",
    "        model = Sequential()\n",
    "\n",
    "        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(ZeroPadding2D(padding=((0, 1), (0, 1))))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "        model.add(BatchNormalization(momentum=0.8))\n",
    "        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n",
    "        model.add(LeakyReLU(alpha=0.2))\n",
    "        model.add(Dropout(0.25))\n",
    "\n",
    "        model.add(Flatten())\n",
    "        model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "        model.summary()\n",
    "\n",
    "        img = Input(shape=img_shape)\n",
    "        validity = model(img)\n",
    "\n",
    "        return Model(img, validity)\n",
    "\n",
    "    def build_combined(self):\n",
    "        self.discriminator.trainable = False\n",
    "        model = Sequential([self.generator, self.discriminator])\n",
    "\n",
    "        return model\n",
    "\n",
    "    def train(self, iterations, batch_size=128, save_interval=50, model_interval=10000, check_noise=None, r=5, c=5):\n",
    "\n",
    "        X_train, labels = self.load_imgs()\n",
    "\n",
    "        half_batch = int(batch_size / 2)\n",
    "\n",
    "        X_train = (X_train.astype(np.float32) - 127.5) / 127.5\n",
    "\n",
    "        for iteration in range(iterations):\n",
    "\n",
    "            # ------------------\n",
    "            # Training Discriminator\n",
    "            # -----------------\n",
    "            idx = np.random.randint(0, X_train.shape[0], half_batch)\n",
    "\n",
    "            imgs = X_train[idx]\n",
    "\n",
    "            noise = np.random.uniform(-1, 1, (half_batch, self.z_dim))\n",
    "\n",
    "            gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "            d_loss_real = self.discriminator.train_on_batch(imgs, np.ones((half_batch, 1)))\n",
    "            d_loss_fake = self.discriminator.train_on_batch(gen_imgs, np.zeros((half_batch, 1)))\n",
    "\n",
    "            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n",
    "\n",
    "            # -----------------\n",
    "            # Training Generator\n",
    "            # -----------------\n",
    "\n",
    "            noise = np.random.uniform(-1, 1, (batch_size, self.z_dim))\n",
    "\n",
    "            g_loss = self.combined.train_on_batch(noise, np.ones((batch_size, 1)))\n",
    "\n",
    "            print(\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\" % (iteration, d_loss[0], 100 * d_loss[1], g_loss))\n",
    "\n",
    "            if iteration % save_interval == 0:\n",
    "                self.save_imgs(iteration, check_noise, r, c)\n",
    "                start = np.expand_dims(check_noise[0], axis=0)\n",
    "                end = np.expand_dims(check_noise[1], axis=0)\n",
    "                resultImage = self.visualizeInterpolation(start=start, end=end)\n",
    "                cv2.imwrite(save_dir + \"latent_{}.png\".format(iteration), resultImage)\n",
    "                if iteration % model_interval == 0:\n",
    "                    self.generator.save(\"mb_dcgan-{}-iter.h5\".format(iteration))\n",
    "\n",
    "    def save_imgs(self, iteration, check_noise, r, c):\n",
    "        noise = check_noise\n",
    "        gen_imgs = self.generator.predict(noise)\n",
    "\n",
    "        # 0-1 rescale\n",
    "        gen_imgs = 0.5 * gen_imgs + 0.5\n",
    "\n",
    "        fig, axs = plt.subplots(r, c)\n",
    "        cnt = 0\n",
    "        for i in range(r):\n",
    "            for j in range(c):\n",
    "                axs[i, j].imshow(gen_imgs[cnt, :, :, :])\n",
    "                axs[i, j].axis('off')\n",
    "                cnt += 1\n",
    "        fig.savefig(save_dir + '%d.png' % iteration)\n",
    "        # fig.savefig('images/gen_imgs/kill_me_%d.png' % iteration)\n",
    "\n",
    "        plt.close()\n",
    "\n",
    "    def load_imgs(self):\n",
    "\n",
    "        img_paths = []\n",
    "        labels = []\n",
    "        images = []\n",
    "\n",
    "    #print(input_img_dir)\n",
    "    #print(self.class_names)\n",
    "\n",
    "        for cl_name in self.class_names:\n",
    "            if cl_name == input_img_dir:\n",
    "                img_names = os.listdir(os.path.join(root_dir, cl_name))\n",
    "\n",
    "\n",
    "\n",
    "                for img_name in img_names:\n",
    "                    img_paths.append(os.path.abspath(os.path.join(root_dir, cl_name, img_name)))\n",
    "                    hot_cl_name = self.get_class_one_hot(cl_name)\n",
    "                    labels.append(hot_cl_name)\n",
    "\n",
    "        for img_path in img_paths:\n",
    "            img = cv2.imread(img_path)\n",
    "            img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "            images.append(img)\n",
    "\n",
    "        images = np.array(images)\n",
    "\n",
    "        return (np.array(images), np.array(labels))\n",
    "\n",
    "    def get_class_one_hot(self, class_str):\n",
    "        label_encoded = self.class_names.index(class_str)\n",
    "\n",
    "        label_hot = np_utils.to_categorical(label_encoded, len(self.class_names))\n",
    "        label_hot = label_hot\n",
    "\n",
    "        return label_hot\n",
    "\n",
    "    def visualizeInterpolation(self, start, end, save=True, nbSteps=10):\n",
    "        print(\"Generating interpolations...\")\n",
    "\n",
    "        steps = nbSteps\n",
    "        latentStart = start\n",
    "        latentEnd = end\n",
    "\n",
    "        startImg = self.generator.predict(latentStart)\n",
    "        endImg = self.generator.predict(latentEnd)\n",
    "\n",
    "        vectors = []\n",
    "\n",
    "        alphaValues = np.linspace(0, 1, steps)\n",
    "        for alpha in alphaValues:\n",
    "            vector = latentStart * (1 - alpha) + latentEnd * alpha\n",
    "            vectors.append(vector)\n",
    "\n",
    "        vectors = np.array(vectors)\n",
    "\n",
    "        resultLatent = None\n",
    "        resultImage = None\n",
    "\n",
    "        for i, vec in enumerate(vectors):\n",
    "            gen_img = np.squeeze(self.generator.predict(vec), axis=0)\n",
    "            gen_img = (0.5 * gen_img + 0.5) * 255\n",
    "            interpolatedImage = cv2.cvtColor(gen_img, cv2.COLOR_RGB2BGR)\n",
    "            interpolatedImage = interpolatedImage.astype(np.uint8)\n",
    "            resultImage = interpolatedImage if resultImage is None else np.hstack([resultImage, interpolatedImage])\n",
    "\n",
    "        return resultImage\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 64, 64, 32)        896       \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 64, 64, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 32, 32, 64)        18496     \n",
      "_________________________________________________________________\n",
      "zero_padding2d (ZeroPadding2 (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 33, 33, 64)        0         \n",
      "_________________________________________________________________\n",
      "batch_normalization (BatchNo (None, 33, 33, 64)        256       \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 17, 17, 128)       73856     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 17, 17, 128)       512       \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 17, 17, 256)       295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 17, 17, 256)       0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 73984)             0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 73985     \n",
      "=================================================================\n",
      "Total params: 463,169\n",
      "Trainable params: 462,785\n",
      "Non-trainable params: 384\n",
      "_________________________________________________________________\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 131072)            13238272  \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 32, 32, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 32, 32, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d (UpSampling2D) (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 64, 64, 128)       147584    \n",
      "_________________________________________________________________\n",
      "activation (Activation)      (None, 64, 64, 128)       0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_3 (Batch (None, 64, 64, 128)       512       \n",
      "_________________________________________________________________\n",
      "up_sampling2d_1 (UpSampling2 (None, 128, 128, 128)     0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 128, 128, 64)      73792     \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 128, 128, 64)      0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_4 (Batch (None, 128, 128, 64)      256       \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 128, 128, 3)       1731      \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 128, 128, 3)       0         \n",
      "=================================================================\n",
      "Total params: 13,462,659\n",
      "Trainable params: 13,462,019\n",
      "Non-trainable params: 640\n",
      "_________________________________________________________________\n",
      "0 [D loss: 1.522631, acc.: 34.38%] [G loss: 0.616550]\n",
      "Generating interpolations...\n",
      "1 [D loss: 0.835756, acc.: 59.38%] [G loss: 0.901768]\n",
      "2 [D loss: 0.692287, acc.: 68.75%] [G loss: 0.918009]\n",
      "3 [D loss: 0.437343, acc.: 75.00%] [G loss: 0.607092]\n",
      "4 [D loss: 0.150755, acc.: 96.88%] [G loss: 0.557170]\n",
      "5 [D loss: 0.143916, acc.: 96.88%] [G loss: 0.276394]\n",
      "6 [D loss: 0.052288, acc.: 100.00%] [G loss: 0.182366]\n",
      "7 [D loss: 0.102345, acc.: 96.88%] [G loss: 0.065664]\n",
      "8 [D loss: 0.081605, acc.: 96.88%] [G loss: 0.036690]\n",
      "9 [D loss: 0.092197, acc.: 100.00%] [G loss: 0.041785]\n",
      "10 [D loss: 0.065847, acc.: 96.88%] [G loss: 0.026890]\n",
      "11 [D loss: 0.106077, acc.: 96.88%] [G loss: 0.003203]\n",
      "12 [D loss: 0.084551, acc.: 100.00%] [G loss: 0.004675]\n",
      "13 [D loss: 0.033538, acc.: 100.00%] [G loss: 0.005774]\n",
      "14 [D loss: 0.027516, acc.: 100.00%] [G loss: 0.014853]\n",
      "15 [D loss: 0.275135, acc.: 90.62%] [G loss: 0.009615]\n",
      "16 [D loss: 0.052817, acc.: 100.00%] [G loss: 0.028060]\n",
      "17 [D loss: 0.160783, acc.: 96.88%] [G loss: 0.002685]\n",
      "18 [D loss: 0.203488, acc.: 87.50%] [G loss: 0.458549]\n",
      "19 [D loss: 0.205122, acc.: 96.88%] [G loss: 0.511502]\n",
      "20 [D loss: 0.676036, acc.: 75.00%] [G loss: 0.703158]\n",
      "21 [D loss: 0.290113, acc.: 87.50%] [G loss: 0.003210]\n",
      "22 [D loss: 0.070169, acc.: 96.88%] [G loss: 0.040355]\n",
      "23 [D loss: 0.101302, acc.: 93.75%] [G loss: 0.495938]\n",
      "24 [D loss: 0.098800, acc.: 96.88%] [G loss: 0.611128]\n",
      "25 [D loss: 0.054848, acc.: 96.88%] [G loss: 0.029632]\n",
      "26 [D loss: 0.313052, acc.: 87.50%] [G loss: 5.148544]\n",
      "27 [D loss: 0.807247, acc.: 59.38%] [G loss: 0.003491]\n",
      "28 [D loss: 0.236625, acc.: 87.50%] [G loss: 0.187252]\n",
      "29 [D loss: 0.018143, acc.: 100.00%] [G loss: 1.257035]\n",
      "30 [D loss: 0.093056, acc.: 96.88%] [G loss: 0.233080]\n",
      "31 [D loss: 0.246412, acc.: 87.50%] [G loss: 2.557680]\n",
      "32 [D loss: 0.089596, acc.: 100.00%] [G loss: 1.691818]\n",
      "33 [D loss: 0.521514, acc.: 75.00%] [G loss: 0.509741]\n",
      "34 [D loss: 0.008985, acc.: 100.00%] [G loss: 1.400140]\n",
      "35 [D loss: 1.933137, acc.: 40.62%] [G loss: 13.145906]\n",
      "36 [D loss: 2.054315, acc.: 53.12%] [G loss: 3.843625]\n",
      "37 [D loss: 0.795536, acc.: 71.88%] [G loss: 5.660723]\n",
      "38 [D loss: 0.005966, acc.: 100.00%] [G loss: 7.875897]\n",
      "39 [D loss: 0.828708, acc.: 71.88%] [G loss: 2.224382]\n",
      "40 [D loss: 0.891628, acc.: 65.62%] [G loss: 8.820973]\n",
      "41 [D loss: 0.831559, acc.: 75.00%] [G loss: 4.932079]\n",
      "42 [D loss: 0.240155, acc.: 81.25%] [G loss: 5.603218]\n",
      "43 [D loss: 0.209438, acc.: 90.62%] [G loss: 3.034717]\n",
      "44 [D loss: 0.067054, acc.: 96.88%] [G loss: 2.482458]\n",
      "45 [D loss: 0.139141, acc.: 96.88%] [G loss: 3.181854]\n",
      "46 [D loss: 0.462716, acc.: 84.38%] [G loss: 2.060070]\n",
      "47 [D loss: 0.007855, acc.: 100.00%] [G loss: 2.999922]\n",
      "48 [D loss: 0.064143, acc.: 100.00%] [G loss: 2.373223]\n",
      "49 [D loss: 0.227087, acc.: 96.88%] [G loss: 2.817532]\n",
      "50 [D loss: 0.051663, acc.: 100.00%] [G loss: 2.751921]\n",
      "Generating interpolations...\n",
      "51 [D loss: 0.120965, acc.: 93.75%] [G loss: 2.051617]\n",
      "52 [D loss: 0.037496, acc.: 100.00%] [G loss: 1.894215]\n",
      "53 [D loss: 0.034637, acc.: 100.00%] [G loss: 1.792137]\n",
      "54 [D loss: 0.017558, acc.: 100.00%] [G loss: 1.299041]\n",
      "55 [D loss: 0.016102, acc.: 100.00%] [G loss: 1.129314]\n",
      "56 [D loss: 0.110447, acc.: 93.75%] [G loss: 1.171109]\n",
      "57 [D loss: 0.030019, acc.: 100.00%] [G loss: 1.194702]\n",
      "58 [D loss: 0.010216, acc.: 100.00%] [G loss: 1.196564]\n",
      "59 [D loss: 0.028744, acc.: 100.00%] [G loss: 1.282621]\n",
      "60 [D loss: 0.036371, acc.: 100.00%] [G loss: 0.734419]\n",
      "61 [D loss: 0.032446, acc.: 100.00%] [G loss: 0.601943]\n",
      "62 [D loss: 0.005030, acc.: 100.00%] [G loss: 0.565208]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63 [D loss: 0.017330, acc.: 100.00%] [G loss: 0.764356]\n",
      "64 [D loss: 0.015066, acc.: 100.00%] [G loss: 0.798455]\n",
      "65 [D loss: 0.039729, acc.: 100.00%] [G loss: 0.485776]\n",
      "66 [D loss: 0.027144, acc.: 100.00%] [G loss: 0.483694]\n",
      "67 [D loss: 0.043833, acc.: 96.88%] [G loss: 0.915500]\n",
      "68 [D loss: 0.009299, acc.: 100.00%] [G loss: 1.804410]\n",
      "69 [D loss: 0.062964, acc.: 100.00%] [G loss: 0.341795]\n",
      "70 [D loss: 0.018482, acc.: 100.00%] [G loss: 0.524709]\n",
      "71 [D loss: 0.017857, acc.: 100.00%] [G loss: 0.494170]\n",
      "72 [D loss: 0.002797, acc.: 100.00%] [G loss: 0.900965]\n",
      "73 [D loss: 0.006360, acc.: 100.00%] [G loss: 0.696527]\n",
      "74 [D loss: 0.028181, acc.: 100.00%] [G loss: 0.320790]\n",
      "75 [D loss: 0.001504, acc.: 100.00%] [G loss: 0.290117]\n",
      "76 [D loss: 0.009201, acc.: 100.00%] [G loss: 0.201150]\n",
      "77 [D loss: 0.007366, acc.: 100.00%] [G loss: 0.341701]\n",
      "78 [D loss: 0.011624, acc.: 100.00%] [G loss: 0.221798]\n",
      "79 [D loss: 0.008285, acc.: 100.00%] [G loss: 0.460667]\n",
      "80 [D loss: 0.021151, acc.: 100.00%] [G loss: 0.176071]\n",
      "81 [D loss: 0.038605, acc.: 100.00%] [G loss: 0.613181]\n",
      "82 [D loss: 0.006643, acc.: 100.00%] [G loss: 0.907106]\n",
      "83 [D loss: 0.008949, acc.: 100.00%] [G loss: 0.527641]\n",
      "84 [D loss: 0.015292, acc.: 100.00%] [G loss: 0.773426]\n",
      "85 [D loss: 0.011623, acc.: 100.00%] [G loss: 0.718492]\n",
      "86 [D loss: 0.009979, acc.: 100.00%] [G loss: 0.384741]\n",
      "87 [D loss: 0.035177, acc.: 100.00%] [G loss: 0.803901]\n",
      "88 [D loss: 0.012024, acc.: 100.00%] [G loss: 0.655271]\n",
      "89 [D loss: 0.047241, acc.: 100.00%] [G loss: 0.519702]\n",
      "90 [D loss: 0.029875, acc.: 100.00%] [G loss: 2.342424]\n",
      "91 [D loss: 0.116132, acc.: 93.75%] [G loss: 1.553439]\n",
      "92 [D loss: 0.429664, acc.: 78.12%] [G loss: 23.861641]\n",
      "93 [D loss: 4.127332, acc.: 50.00%] [G loss: 0.058838]\n",
      "94 [D loss: 2.009233, acc.: 50.00%] [G loss: 22.166412]\n",
      "95 [D loss: 0.000011, acc.: 100.00%] [G loss: 24.731228]\n",
      "96 [D loss: 0.133861, acc.: 93.75%] [G loss: 9.130763]\n",
      "97 [D loss: 1.399735, acc.: 56.25%] [G loss: 26.687389]\n",
      "98 [D loss: 4.685177, acc.: 53.12%] [G loss: 12.217110]\n",
      "99 [D loss: 0.501306, acc.: 81.25%] [G loss: 11.728290]\n",
      "100 [D loss: 0.956749, acc.: 78.12%] [G loss: 9.245913]\n",
      "Generating interpolations...\n",
      "101 [D loss: 1.682302, acc.: 71.88%] [G loss: 9.286800]\n",
      "102 [D loss: 1.283983, acc.: 62.50%] [G loss: 5.042040]\n",
      "103 [D loss: 0.132584, acc.: 93.75%] [G loss: 7.147325]\n",
      "104 [D loss: 2.637954, acc.: 25.00%] [G loss: 8.794378]\n",
      "105 [D loss: 0.076652, acc.: 96.88%] [G loss: 12.168125]\n",
      "106 [D loss: 1.396254, acc.: 56.25%] [G loss: 0.907337]\n",
      "107 [D loss: 2.080031, acc.: 53.12%] [G loss: 11.614918]\n",
      "108 [D loss: 0.453791, acc.: 87.50%] [G loss: 13.086046]\n",
      "109 [D loss: 1.299837, acc.: 59.38%] [G loss: 1.135934]\n",
      "110 [D loss: 1.346468, acc.: 62.50%] [G loss: 5.057054]\n",
      "111 [D loss: 0.017575, acc.: 100.00%] [G loss: 10.477242]\n",
      "112 [D loss: 0.449146, acc.: 78.12%] [G loss: 1.770604]\n",
      "113 [D loss: 0.484880, acc.: 81.25%] [G loss: 4.475319]\n",
      "114 [D loss: 0.050457, acc.: 96.88%] [G loss: 6.904679]\n",
      "115 [D loss: 0.322243, acc.: 84.38%] [G loss: 0.609388]\n",
      "116 [D loss: 0.522605, acc.: 75.00%] [G loss: 4.462147]\n",
      "117 [D loss: 0.256226, acc.: 93.75%] [G loss: 5.144102]\n",
      "118 [D loss: 0.031894, acc.: 100.00%] [G loss: 3.033721]\n",
      "119 [D loss: 0.084051, acc.: 96.88%] [G loss: 1.019842]\n",
      "120 [D loss: 0.031708, acc.: 100.00%] [G loss: 0.766107]\n",
      "121 [D loss: 0.046488, acc.: 100.00%] [G loss: 0.767496]\n",
      "122 [D loss: 0.023424, acc.: 100.00%] [G loss: 0.665827]\n",
      "123 [D loss: 0.010562, acc.: 100.00%] [G loss: 0.934287]\n",
      "124 [D loss: 0.018891, acc.: 100.00%] [G loss: 0.504696]\n",
      "125 [D loss: 0.043232, acc.: 100.00%] [G loss: 0.415065]\n",
      "126 [D loss: 0.121119, acc.: 96.88%] [G loss: 0.892267]\n",
      "127 [D loss: 0.050293, acc.: 96.88%] [G loss: 1.420459]\n",
      "128 [D loss: 0.018940, acc.: 100.00%] [G loss: 1.522367]\n",
      "129 [D loss: 0.098196, acc.: 96.88%] [G loss: 0.371138]\n",
      "130 [D loss: 0.093316, acc.: 96.88%] [G loss: 1.127279]\n",
      "131 [D loss: 0.076034, acc.: 96.88%] [G loss: 0.673152]\n",
      "132 [D loss: 0.019196, acc.: 100.00%] [G loss: 0.585901]\n",
      "133 [D loss: 0.057144, acc.: 100.00%] [G loss: 0.335977]\n",
      "134 [D loss: 0.052439, acc.: 100.00%] [G loss: 0.491373]\n",
      "135 [D loss: 0.006638, acc.: 100.00%] [G loss: 0.585190]\n",
      "136 [D loss: 0.023434, acc.: 100.00%] [G loss: 0.398417]\n",
      "137 [D loss: 0.125963, acc.: 96.88%] [G loss: 0.487348]\n",
      "138 [D loss: 0.182253, acc.: 90.62%] [G loss: 0.963675]\n",
      "139 [D loss: 0.002924, acc.: 100.00%] [G loss: 0.714898]\n",
      "140 [D loss: 0.005475, acc.: 100.00%] [G loss: 0.767210]\n",
      "141 [D loss: 0.030862, acc.: 100.00%] [G loss: 0.189028]\n",
      "142 [D loss: 0.011081, acc.: 100.00%] [G loss: 0.237994]\n",
      "143 [D loss: 0.065880, acc.: 100.00%] [G loss: 0.162182]\n",
      "144 [D loss: 0.007007, acc.: 100.00%] [G loss: 0.378272]\n",
      "145 [D loss: 0.003249, acc.: 100.00%] [G loss: 0.575392]\n",
      "146 [D loss: 0.002481, acc.: 100.00%] [G loss: 0.237796]\n",
      "147 [D loss: 0.003811, acc.: 100.00%] [G loss: 0.181289]\n",
      "148 [D loss: 0.002502, acc.: 100.00%] [G loss: 0.200388]\n",
      "149 [D loss: 0.002644, acc.: 100.00%] [G loss: 0.124462]\n",
      "150 [D loss: 0.003322, acc.: 100.00%] [G loss: 0.364466]\n",
      "Generating interpolations...\n",
      "151 [D loss: 0.002541, acc.: 100.00%] [G loss: 0.198642]\n",
      "152 [D loss: 0.057811, acc.: 96.88%] [G loss: 0.051660]\n",
      "153 [D loss: 0.016773, acc.: 100.00%] [G loss: 0.069043]\n",
      "154 [D loss: 0.001321, acc.: 100.00%] [G loss: 0.098048]\n",
      "155 [D loss: 0.002777, acc.: 100.00%] [G loss: 0.073487]\n",
      "156 [D loss: 0.019795, acc.: 100.00%] [G loss: 0.068519]\n",
      "157 [D loss: 0.001160, acc.: 100.00%] [G loss: 0.061214]\n",
      "158 [D loss: 0.001558, acc.: 100.00%] [G loss: 0.113977]\n",
      "159 [D loss: 0.000528, acc.: 100.00%] [G loss: 0.066133]\n",
      "160 [D loss: 0.009986, acc.: 100.00%] [G loss: 0.092306]\n",
      "161 [D loss: 0.001575, acc.: 100.00%] [G loss: 0.088261]\n",
      "162 [D loss: 0.003913, acc.: 100.00%] [G loss: 0.082460]\n",
      "163 [D loss: 0.007731, acc.: 100.00%] [G loss: 0.057488]\n",
      "164 [D loss: 0.001118, acc.: 100.00%] [G loss: 0.032027]\n",
      "165 [D loss: 0.001022, acc.: 100.00%] [G loss: 0.119123]\n",
      "166 [D loss: 0.004050, acc.: 100.00%] [G loss: 0.134818]\n",
      "167 [D loss: 0.000518, acc.: 100.00%] [G loss: 0.068049]\n",
      "168 [D loss: 0.002648, acc.: 100.00%] [G loss: 0.052924]\n",
      "169 [D loss: 0.002358, acc.: 100.00%] [G loss: 0.061479]\n",
      "170 [D loss: 0.001384, acc.: 100.00%] [G loss: 0.117455]\n",
      "171 [D loss: 0.003259, acc.: 100.00%] [G loss: 0.102281]\n",
      "172 [D loss: 0.004628, acc.: 100.00%] [G loss: 0.113209]\n",
      "173 [D loss: 0.000709, acc.: 100.00%] [G loss: 0.045543]\n",
      "174 [D loss: 0.008769, acc.: 100.00%] [G loss: 0.096601]\n",
      "175 [D loss: 0.001548, acc.: 100.00%] [G loss: 0.062964]\n",
      "176 [D loss: 0.001227, acc.: 100.00%] [G loss: 0.081504]\n",
      "177 [D loss: 0.000805, acc.: 100.00%] [G loss: 0.050037]\n",
      "178 [D loss: 0.002392, acc.: 100.00%] [G loss: 0.090360]\n",
      "179 [D loss: 0.001533, acc.: 100.00%] [G loss: 0.064305]\n",
      "180 [D loss: 0.001108, acc.: 100.00%] [G loss: 0.115294]\n",
      "181 [D loss: 0.011215, acc.: 100.00%] [G loss: 0.106268]\n",
      "182 [D loss: 0.001537, acc.: 100.00%] [G loss: 0.187776]\n",
      "183 [D loss: 0.005785, acc.: 100.00%] [G loss: 0.086847]\n",
      "184 [D loss: 0.001323, acc.: 100.00%] [G loss: 0.054838]\n",
      "185 [D loss: 0.000544, acc.: 100.00%] [G loss: 0.066950]\n",
      "186 [D loss: 0.000558, acc.: 100.00%] [G loss: 0.066966]\n",
      "187 [D loss: 0.000744, acc.: 100.00%] [G loss: 0.060085]\n",
      "188 [D loss: 0.002560, acc.: 100.00%] [G loss: 0.041441]\n",
      "189 [D loss: 0.002545, acc.: 100.00%] [G loss: 0.148302]\n",
      "190 [D loss: 0.000394, acc.: 100.00%] [G loss: 0.035750]\n",
      "191 [D loss: 0.008862, acc.: 100.00%] [G loss: 0.045915]\n",
      "192 [D loss: 0.005333, acc.: 100.00%] [G loss: 0.068651]\n",
      "193 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.052301]\n",
      "194 [D loss: 0.002170, acc.: 100.00%] [G loss: 0.058708]\n",
      "195 [D loss: 0.000382, acc.: 100.00%] [G loss: 0.031802]\n",
      "196 [D loss: 0.000562, acc.: 100.00%] [G loss: 0.037330]\n",
      "197 [D loss: 0.007117, acc.: 100.00%] [G loss: 0.061744]\n",
      "198 [D loss: 0.001354, acc.: 100.00%] [G loss: 0.057628]\n",
      "199 [D loss: 0.001815, acc.: 100.00%] [G loss: 0.042248]\n",
      "200 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.038148]\n",
      "Generating interpolations...\n",
      "201 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.051314]\n",
      "202 [D loss: 0.001982, acc.: 100.00%] [G loss: 0.016399]\n",
      "203 [D loss: 0.003233, acc.: 100.00%] [G loss: 0.029074]\n",
      "204 [D loss: 0.000879, acc.: 100.00%] [G loss: 0.018637]\n",
      "205 [D loss: 0.000996, acc.: 100.00%] [G loss: 0.014242]\n",
      "206 [D loss: 0.004086, acc.: 100.00%] [G loss: 0.018852]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "207 [D loss: 0.001701, acc.: 100.00%] [G loss: 0.026423]\n",
      "208 [D loss: 0.000996, acc.: 100.00%] [G loss: 0.073286]\n",
      "209 [D loss: 0.006339, acc.: 100.00%] [G loss: 0.038605]\n",
      "210 [D loss: 0.008079, acc.: 100.00%] [G loss: 0.064572]\n",
      "211 [D loss: 0.000256, acc.: 100.00%] [G loss: 0.108567]\n",
      "212 [D loss: 0.000872, acc.: 100.00%] [G loss: 0.044599]\n",
      "213 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.084168]\n",
      "214 [D loss: 0.000324, acc.: 100.00%] [G loss: 0.015773]\n",
      "215 [D loss: 0.000243, acc.: 100.00%] [G loss: 0.031833]\n",
      "216 [D loss: 0.002416, acc.: 100.00%] [G loss: 0.045668]\n",
      "217 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.045484]\n",
      "218 [D loss: 0.000644, acc.: 100.00%] [G loss: 0.045510]\n",
      "219 [D loss: 0.001267, acc.: 100.00%] [G loss: 0.024425]\n",
      "220 [D loss: 0.003565, acc.: 100.00%] [G loss: 0.092482]\n",
      "221 [D loss: 0.000746, acc.: 100.00%] [G loss: 0.100813]\n",
      "222 [D loss: 0.000813, acc.: 100.00%] [G loss: 0.035519]\n",
      "223 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.043020]\n",
      "224 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.014977]\n",
      "225 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.042482]\n",
      "226 [D loss: 0.000609, acc.: 100.00%] [G loss: 0.040447]\n",
      "227 [D loss: 0.000813, acc.: 100.00%] [G loss: 0.033548]\n",
      "228 [D loss: 0.001912, acc.: 100.00%] [G loss: 0.024064]\n",
      "229 [D loss: 0.004690, acc.: 100.00%] [G loss: 0.076021]\n",
      "230 [D loss: 0.000870, acc.: 100.00%] [G loss: 0.054405]\n",
      "231 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.032975]\n",
      "232 [D loss: 0.000749, acc.: 100.00%] [G loss: 0.029895]\n",
      "233 [D loss: 0.000750, acc.: 100.00%] [G loss: 0.058065]\n",
      "234 [D loss: 0.000987, acc.: 100.00%] [G loss: 0.038080]\n",
      "235 [D loss: 0.001440, acc.: 100.00%] [G loss: 0.042618]\n",
      "236 [D loss: 0.000739, acc.: 100.00%] [G loss: 0.041441]\n",
      "237 [D loss: 0.000242, acc.: 100.00%] [G loss: 0.018405]\n",
      "238 [D loss: 0.002975, acc.: 100.00%] [G loss: 0.012387]\n",
      "239 [D loss: 0.002831, acc.: 100.00%] [G loss: 0.021324]\n",
      "240 [D loss: 0.001677, acc.: 100.00%] [G loss: 0.042824]\n",
      "241 [D loss: 0.000299, acc.: 100.00%] [G loss: 0.045540]\n",
      "242 [D loss: 0.000530, acc.: 100.00%] [G loss: 0.035918]\n",
      "243 [D loss: 0.003200, acc.: 100.00%] [G loss: 0.139369]\n",
      "244 [D loss: 0.001299, acc.: 100.00%] [G loss: 0.076140]\n",
      "245 [D loss: 0.000547, acc.: 100.00%] [G loss: 0.017854]\n",
      "246 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.032177]\n",
      "247 [D loss: 0.000674, acc.: 100.00%] [G loss: 0.019142]\n",
      "248 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.017176]\n",
      "249 [D loss: 0.000334, acc.: 100.00%] [G loss: 0.014285]\n",
      "250 [D loss: 0.000410, acc.: 100.00%] [G loss: 0.024674]\n",
      "Generating interpolations...\n",
      "251 [D loss: 0.000274, acc.: 100.00%] [G loss: 0.016884]\n",
      "252 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.017584]\n",
      "253 [D loss: 0.000667, acc.: 100.00%] [G loss: 0.015639]\n",
      "254 [D loss: 0.000182, acc.: 100.00%] [G loss: 0.015339]\n",
      "255 [D loss: 0.001142, acc.: 100.00%] [G loss: 0.009500]\n",
      "256 [D loss: 0.000954, acc.: 100.00%] [G loss: 0.034572]\n",
      "257 [D loss: 0.000652, acc.: 100.00%] [G loss: 0.013220]\n",
      "258 [D loss: 0.000826, acc.: 100.00%] [G loss: 0.011538]\n",
      "259 [D loss: 0.000627, acc.: 100.00%] [G loss: 0.014298]\n",
      "260 [D loss: 0.000196, acc.: 100.00%] [G loss: 0.026156]\n",
      "261 [D loss: 0.003298, acc.: 100.00%] [G loss: 0.027167]\n",
      "262 [D loss: 0.001621, acc.: 100.00%] [G loss: 0.036744]\n",
      "263 [D loss: 0.000234, acc.: 100.00%] [G loss: 0.021117]\n",
      "264 [D loss: 0.000560, acc.: 100.00%] [G loss: 0.020351]\n",
      "265 [D loss: 0.000586, acc.: 100.00%] [G loss: 0.030884]\n",
      "266 [D loss: 0.000837, acc.: 100.00%] [G loss: 0.020908]\n",
      "267 [D loss: 0.001330, acc.: 100.00%] [G loss: 0.019759]\n",
      "268 [D loss: 0.001146, acc.: 100.00%] [G loss: 0.019481]\n",
      "269 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.021434]\n",
      "270 [D loss: 0.000689, acc.: 100.00%] [G loss: 0.055420]\n",
      "271 [D loss: 0.000677, acc.: 100.00%] [G loss: 0.007548]\n",
      "272 [D loss: 0.000617, acc.: 100.00%] [G loss: 0.025141]\n",
      "273 [D loss: 0.000403, acc.: 100.00%] [G loss: 0.018289]\n",
      "274 [D loss: 0.000842, acc.: 100.00%] [G loss: 0.023206]\n",
      "275 [D loss: 0.000286, acc.: 100.00%] [G loss: 0.016119]\n",
      "276 [D loss: 0.000988, acc.: 100.00%] [G loss: 0.018181]\n",
      "277 [D loss: 0.000699, acc.: 100.00%] [G loss: 0.014898]\n",
      "278 [D loss: 0.000976, acc.: 100.00%] [G loss: 0.051339]\n",
      "279 [D loss: 0.001512, acc.: 100.00%] [G loss: 0.015921]\n",
      "280 [D loss: 0.000436, acc.: 100.00%] [G loss: 0.016096]\n",
      "281 [D loss: 0.000662, acc.: 100.00%] [G loss: 0.022806]\n",
      "282 [D loss: 0.001048, acc.: 100.00%] [G loss: 0.015348]\n",
      "283 [D loss: 0.000457, acc.: 100.00%] [G loss: 0.029824]\n",
      "284 [D loss: 0.001854, acc.: 100.00%] [G loss: 0.013739]\n",
      "285 [D loss: 0.000520, acc.: 100.00%] [G loss: 0.018625]\n",
      "286 [D loss: 0.000393, acc.: 100.00%] [G loss: 0.026856]\n",
      "287 [D loss: 0.000785, acc.: 100.00%] [G loss: 0.026186]\n",
      "288 [D loss: 0.000462, acc.: 100.00%] [G loss: 0.017365]\n",
      "289 [D loss: 0.000414, acc.: 100.00%] [G loss: 0.014482]\n",
      "290 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.014035]\n",
      "291 [D loss: 0.000492, acc.: 100.00%] [G loss: 0.014801]\n",
      "292 [D loss: 0.000130, acc.: 100.00%] [G loss: 0.010163]\n",
      "293 [D loss: 0.000968, acc.: 100.00%] [G loss: 0.023583]\n",
      "294 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.015169]\n",
      "295 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.024764]\n",
      "296 [D loss: 0.000319, acc.: 100.00%] [G loss: 0.037266]\n",
      "297 [D loss: 0.000916, acc.: 100.00%] [G loss: 0.013763]\n",
      "298 [D loss: 0.000933, acc.: 100.00%] [G loss: 0.028954]\n",
      "299 [D loss: 0.000430, acc.: 100.00%] [G loss: 0.020951]\n",
      "300 [D loss: 0.001176, acc.: 100.00%] [G loss: 0.021591]\n",
      "Generating interpolations...\n",
      "301 [D loss: 0.002368, acc.: 100.00%] [G loss: 0.022191]\n",
      "302 [D loss: 0.000568, acc.: 100.00%] [G loss: 0.020866]\n",
      "303 [D loss: 0.000438, acc.: 100.00%] [G loss: 0.008222]\n",
      "304 [D loss: 0.002387, acc.: 100.00%] [G loss: 0.016281]\n",
      "305 [D loss: 0.003691, acc.: 100.00%] [G loss: 0.009308]\n",
      "306 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.014343]\n",
      "307 [D loss: 0.000220, acc.: 100.00%] [G loss: 0.012407]\n",
      "308 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.027649]\n",
      "309 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.024125]\n",
      "310 [D loss: 0.001277, acc.: 100.00%] [G loss: 0.021261]\n",
      "311 [D loss: 0.000810, acc.: 100.00%] [G loss: 0.019904]\n",
      "312 [D loss: 0.000392, acc.: 100.00%] [G loss: 0.018525]\n",
      "313 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.011312]\n",
      "314 [D loss: 0.000227, acc.: 100.00%] [G loss: 0.032537]\n",
      "315 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.016931]\n",
      "316 [D loss: 0.000535, acc.: 100.00%] [G loss: 0.025132]\n",
      "317 [D loss: 0.000217, acc.: 100.00%] [G loss: 0.014334]\n",
      "318 [D loss: 0.000385, acc.: 100.00%] [G loss: 0.008957]\n",
      "319 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.028624]\n",
      "320 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.014192]\n",
      "321 [D loss: 0.000572, acc.: 100.00%] [G loss: 0.016520]\n",
      "322 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.028111]\n",
      "323 [D loss: 0.000247, acc.: 100.00%] [G loss: 0.014541]\n",
      "324 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.009591]\n",
      "325 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.012682]\n",
      "326 [D loss: 0.000422, acc.: 100.00%] [G loss: 0.011126]\n",
      "327 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.016115]\n",
      "328 [D loss: 0.000275, acc.: 100.00%] [G loss: 0.024325]\n",
      "329 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.013853]\n",
      "330 [D loss: 0.001724, acc.: 100.00%] [G loss: 0.036649]\n",
      "331 [D loss: 0.000330, acc.: 100.00%] [G loss: 0.009283]\n",
      "332 [D loss: 0.000378, acc.: 100.00%] [G loss: 0.017317]\n",
      "333 [D loss: 0.000344, acc.: 100.00%] [G loss: 0.036582]\n",
      "334 [D loss: 0.000760, acc.: 100.00%] [G loss: 0.022830]\n",
      "335 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.026763]\n",
      "336 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.028087]\n",
      "337 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.012423]\n",
      "338 [D loss: 0.000279, acc.: 100.00%] [G loss: 0.013311]\n",
      "339 [D loss: 0.000273, acc.: 100.00%] [G loss: 0.022213]\n",
      "340 [D loss: 0.000348, acc.: 100.00%] [G loss: 0.015654]\n",
      "341 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.024484]\n",
      "342 [D loss: 0.000315, acc.: 100.00%] [G loss: 0.016324]\n",
      "343 [D loss: 0.000656, acc.: 100.00%] [G loss: 0.033253]\n",
      "344 [D loss: 0.000440, acc.: 100.00%] [G loss: 0.032559]\n",
      "345 [D loss: 0.000204, acc.: 100.00%] [G loss: 0.023805]\n",
      "346 [D loss: 0.001547, acc.: 100.00%] [G loss: 0.007937]\n",
      "347 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.008337]\n",
      "348 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.016443]\n",
      "349 [D loss: 0.000318, acc.: 100.00%] [G loss: 0.015605]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "350 [D loss: 0.000295, acc.: 100.00%] [G loss: 0.008667]\n",
      "Generating interpolations...\n",
      "351 [D loss: 0.000417, acc.: 100.00%] [G loss: 0.004841]\n",
      "352 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.012774]\n",
      "353 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.002594]\n",
      "354 [D loss: 0.000587, acc.: 100.00%] [G loss: 0.018602]\n",
      "355 [D loss: 0.000194, acc.: 100.00%] [G loss: 0.015714]\n",
      "356 [D loss: 0.000681, acc.: 100.00%] [G loss: 0.006267]\n",
      "357 [D loss: 0.000308, acc.: 100.00%] [G loss: 0.006781]\n",
      "358 [D loss: 0.000266, acc.: 100.00%] [G loss: 0.011611]\n",
      "359 [D loss: 0.000569, acc.: 100.00%] [G loss: 0.019251]\n",
      "360 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.005472]\n",
      "361 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.018310]\n",
      "362 [D loss: 0.000438, acc.: 100.00%] [G loss: 0.012773]\n",
      "363 [D loss: 0.000401, acc.: 100.00%] [G loss: 0.011930]\n",
      "364 [D loss: 0.000578, acc.: 100.00%] [G loss: 0.008992]\n",
      "365 [D loss: 0.000563, acc.: 100.00%] [G loss: 0.010943]\n",
      "366 [D loss: 0.000681, acc.: 100.00%] [G loss: 0.013031]\n",
      "367 [D loss: 0.001342, acc.: 100.00%] [G loss: 0.026868]\n",
      "368 [D loss: 0.000251, acc.: 100.00%] [G loss: 0.020317]\n",
      "369 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.012772]\n",
      "370 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.018449]\n",
      "371 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.008143]\n",
      "372 [D loss: 0.000566, acc.: 100.00%] [G loss: 0.025738]\n",
      "373 [D loss: 0.000729, acc.: 100.00%] [G loss: 0.008584]\n",
      "374 [D loss: 0.000839, acc.: 100.00%] [G loss: 0.023214]\n",
      "375 [D loss: 0.000252, acc.: 100.00%] [G loss: 0.007615]\n",
      "376 [D loss: 0.000544, acc.: 100.00%] [G loss: 0.011088]\n",
      "377 [D loss: 0.000492, acc.: 100.00%] [G loss: 0.029992]\n",
      "378 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.022763]\n",
      "379 [D loss: 0.000345, acc.: 100.00%] [G loss: 0.015170]\n",
      "380 [D loss: 0.000784, acc.: 100.00%] [G loss: 0.017388]\n",
      "381 [D loss: 0.000484, acc.: 100.00%] [G loss: 0.027844]\n",
      "382 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.035822]\n",
      "383 [D loss: 0.000161, acc.: 100.00%] [G loss: 0.016851]\n",
      "384 [D loss: 0.000206, acc.: 100.00%] [G loss: 0.011783]\n",
      "385 [D loss: 0.000616, acc.: 100.00%] [G loss: 0.021677]\n",
      "386 [D loss: 0.000155, acc.: 100.00%] [G loss: 0.011180]\n",
      "387 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.005701]\n",
      "388 [D loss: 0.000232, acc.: 100.00%] [G loss: 0.008903]\n",
      "389 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.017049]\n",
      "390 [D loss: 0.000490, acc.: 100.00%] [G loss: 0.016045]\n",
      "391 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.013791]\n",
      "392 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.022865]\n",
      "393 [D loss: 0.000922, acc.: 100.00%] [G loss: 0.009002]\n",
      "394 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.015482]\n",
      "395 [D loss: 0.000381, acc.: 100.00%] [G loss: 0.017533]\n",
      "396 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.023675]\n",
      "397 [D loss: 0.000297, acc.: 100.00%] [G loss: 0.012295]\n",
      "398 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.028665]\n",
      "399 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.035523]\n",
      "400 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.011219]\n",
      "Generating interpolations...\n",
      "401 [D loss: 0.000143, acc.: 100.00%] [G loss: 0.022694]\n",
      "402 [D loss: 0.000075, acc.: 100.00%] [G loss: 0.017320]\n",
      "403 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.021813]\n",
      "404 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.010304]\n",
      "405 [D loss: 0.002233, acc.: 100.00%] [G loss: 0.008366]\n",
      "406 [D loss: 0.000607, acc.: 100.00%] [G loss: 0.046594]\n",
      "407 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.027490]\n",
      "408 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.043995]\n",
      "409 [D loss: 0.000306, acc.: 100.00%] [G loss: 0.018076]\n",
      "410 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.024965]\n",
      "411 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.007641]\n",
      "412 [D loss: 0.000439, acc.: 100.00%] [G loss: 0.026025]\n",
      "413 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.007757]\n",
      "414 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.013855]\n",
      "415 [D loss: 0.000651, acc.: 100.00%] [G loss: 0.026999]\n",
      "416 [D loss: 0.000342, acc.: 100.00%] [G loss: 0.016123]\n",
      "417 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.032118]\n",
      "418 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.010652]\n",
      "419 [D loss: 0.000816, acc.: 100.00%] [G loss: 0.017469]\n",
      "420 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.010539]\n",
      "421 [D loss: 0.000822, acc.: 100.00%] [G loss: 0.021870]\n",
      "422 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.010570]\n",
      "423 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.016727]\n",
      "424 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.011179]\n",
      "425 [D loss: 0.000333, acc.: 100.00%] [G loss: 0.011387]\n",
      "426 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.006724]\n",
      "427 [D loss: 0.000645, acc.: 100.00%] [G loss: 0.013636]\n",
      "428 [D loss: 0.001610, acc.: 100.00%] [G loss: 0.022586]\n",
      "429 [D loss: 0.000761, acc.: 100.00%] [G loss: 0.018536]\n",
      "430 [D loss: 0.000226, acc.: 100.00%] [G loss: 0.010992]\n",
      "431 [D loss: 0.000509, acc.: 100.00%] [G loss: 0.006015]\n",
      "432 [D loss: 0.000158, acc.: 100.00%] [G loss: 0.006353]\n",
      "433 [D loss: 0.000267, acc.: 100.00%] [G loss: 0.009315]\n",
      "434 [D loss: 0.000203, acc.: 100.00%] [G loss: 0.026234]\n",
      "435 [D loss: 0.000436, acc.: 100.00%] [G loss: 0.006127]\n",
      "436 [D loss: 0.000776, acc.: 100.00%] [G loss: 0.009657]\n",
      "437 [D loss: 0.002057, acc.: 100.00%] [G loss: 0.016548]\n",
      "438 [D loss: 0.001088, acc.: 100.00%] [G loss: 0.008410]\n",
      "439 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.004898]\n",
      "440 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.014376]\n",
      "441 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.006502]\n",
      "442 [D loss: 0.000213, acc.: 100.00%] [G loss: 0.005278]\n",
      "443 [D loss: 0.000686, acc.: 100.00%] [G loss: 0.003395]\n",
      "444 [D loss: 0.000271, acc.: 100.00%] [G loss: 0.005549]\n",
      "445 [D loss: 0.000689, acc.: 100.00%] [G loss: 0.005917]\n",
      "446 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.006527]\n",
      "447 [D loss: 0.000139, acc.: 100.00%] [G loss: 0.003790]\n",
      "448 [D loss: 0.000120, acc.: 100.00%] [G loss: 0.005726]\n",
      "449 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.004969]\n",
      "450 [D loss: 0.000323, acc.: 100.00%] [G loss: 0.022140]\n",
      "Generating interpolations...\n",
      "451 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.003794]\n",
      "452 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.009025]\n",
      "453 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.009118]\n",
      "454 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.007374]\n",
      "455 [D loss: 0.000622, acc.: 100.00%] [G loss: 0.026713]\n",
      "456 [D loss: 0.000230, acc.: 100.00%] [G loss: 0.009732]\n",
      "457 [D loss: 0.000134, acc.: 100.00%] [G loss: 0.012890]\n",
      "458 [D loss: 0.000214, acc.: 100.00%] [G loss: 0.004140]\n",
      "459 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.005396]\n",
      "460 [D loss: 0.000320, acc.: 100.00%] [G loss: 0.007314]\n",
      "461 [D loss: 0.000103, acc.: 100.00%] [G loss: 0.007260]\n",
      "462 [D loss: 0.000296, acc.: 100.00%] [G loss: 0.015784]\n",
      "463 [D loss: 0.000594, acc.: 100.00%] [G loss: 0.005335]\n",
      "464 [D loss: 0.000180, acc.: 100.00%] [G loss: 0.007357]\n",
      "465 [D loss: 0.000362, acc.: 100.00%] [G loss: 0.008285]\n",
      "466 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.005189]\n",
      "467 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.007472]\n",
      "468 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.003771]\n",
      "469 [D loss: 0.000175, acc.: 100.00%] [G loss: 0.009121]\n",
      "470 [D loss: 0.000224, acc.: 100.00%] [G loss: 0.006056]\n",
      "471 [D loss: 0.000268, acc.: 100.00%] [G loss: 0.004877]\n",
      "472 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.012957]\n",
      "473 [D loss: 0.000301, acc.: 100.00%] [G loss: 0.009961]\n",
      "474 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.016778]\n",
      "475 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.009214]\n",
      "476 [D loss: 0.000153, acc.: 100.00%] [G loss: 0.007652]\n",
      "477 [D loss: 0.000438, acc.: 100.00%] [G loss: 0.021358]\n",
      "478 [D loss: 0.000460, acc.: 100.00%] [G loss: 0.008321]\n",
      "479 [D loss: 0.000178, acc.: 100.00%] [G loss: 0.078769]\n",
      "480 [D loss: 0.000302, acc.: 100.00%] [G loss: 0.008472]\n",
      "481 [D loss: 0.000281, acc.: 100.00%] [G loss: 0.007697]\n",
      "482 [D loss: 0.000482, acc.: 100.00%] [G loss: 0.008249]\n",
      "483 [D loss: 0.000276, acc.: 100.00%] [G loss: 0.013795]\n",
      "484 [D loss: 0.000800, acc.: 100.00%] [G loss: 0.019480]\n",
      "485 [D loss: 0.000753, acc.: 100.00%] [G loss: 0.010021]\n",
      "486 [D loss: 0.000218, acc.: 100.00%] [G loss: 0.007366]\n",
      "487 [D loss: 0.000096, acc.: 100.00%] [G loss: 0.003611]\n",
      "488 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.006596]\n",
      "489 [D loss: 0.000186, acc.: 100.00%] [G loss: 0.009451]\n",
      "490 [D loss: 0.000222, acc.: 100.00%] [G loss: 0.008738]\n",
      "491 [D loss: 0.000432, acc.: 100.00%] [G loss: 0.016230]\n",
      "492 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.005330]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "493 [D loss: 0.000667, acc.: 100.00%] [G loss: 0.014207]\n",
      "494 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.006085]\n",
      "495 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.003195]\n",
      "496 [D loss: 0.000777, acc.: 100.00%] [G loss: 0.013100]\n",
      "497 [D loss: 0.000215, acc.: 100.00%] [G loss: 0.013206]\n",
      "498 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.012449]\n",
      "499 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.014420]\n",
      "500 [D loss: 0.000127, acc.: 100.00%] [G loss: 0.007163]\n",
      "Generating interpolations...\n",
      "501 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.006650]\n",
      "502 [D loss: 0.000200, acc.: 100.00%] [G loss: 0.003754]\n",
      "503 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.011326]\n",
      "504 [D loss: 0.000138, acc.: 100.00%] [G loss: 0.008070]\n",
      "505 [D loss: 0.000716, acc.: 100.00%] [G loss: 0.008656]\n",
      "506 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.007202]\n",
      "507 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.013572]\n",
      "508 [D loss: 0.000181, acc.: 100.00%] [G loss: 0.006720]\n",
      "509 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.020189]\n",
      "510 [D loss: 0.000188, acc.: 100.00%] [G loss: 0.010240]\n",
      "511 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.009011]\n",
      "512 [D loss: 0.001237, acc.: 100.00%] [G loss: 0.009929]\n",
      "513 [D loss: 0.000062, acc.: 100.00%] [G loss: 0.006630]\n",
      "514 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.009046]\n",
      "515 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.004096]\n",
      "516 [D loss: 0.000159, acc.: 100.00%] [G loss: 0.003671]\n",
      "517 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.007595]\n",
      "518 [D loss: 0.000258, acc.: 100.00%] [G loss: 0.003975]\n",
      "519 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.007865]\n",
      "520 [D loss: 0.000706, acc.: 100.00%] [G loss: 0.004176]\n",
      "521 [D loss: 0.000166, acc.: 100.00%] [G loss: 0.013429]\n",
      "522 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.005106]\n",
      "523 [D loss: 0.000288, acc.: 100.00%] [G loss: 0.012346]\n",
      "524 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.008356]\n",
      "525 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.021286]\n",
      "526 [D loss: 0.000107, acc.: 100.00%] [G loss: 0.004301]\n",
      "527 [D loss: 0.000089, acc.: 100.00%] [G loss: 0.006629]\n",
      "528 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.007055]\n",
      "529 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.008704]\n",
      "530 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.003674]\n",
      "531 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.005132]\n",
      "532 [D loss: 0.000233, acc.: 100.00%] [G loss: 0.008545]\n",
      "533 [D loss: 0.000185, acc.: 100.00%] [G loss: 0.007476]\n",
      "534 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.006562]\n",
      "535 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.006407]\n",
      "536 [D loss: 0.000678, acc.: 100.00%] [G loss: 0.005635]\n",
      "537 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.007052]\n",
      "538 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.003497]\n",
      "539 [D loss: 0.000152, acc.: 100.00%] [G loss: 0.003547]\n",
      "540 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.015725]\n",
      "541 [D loss: 0.000294, acc.: 100.00%] [G loss: 0.010738]\n",
      "542 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.006048]\n",
      "543 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.004248]\n",
      "544 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.007056]\n",
      "545 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.008888]\n",
      "546 [D loss: 0.000167, acc.: 100.00%] [G loss: 0.002967]\n",
      "547 [D loss: 0.000114, acc.: 100.00%] [G loss: 0.011649]\n",
      "548 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.013568]\n",
      "549 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.009369]\n",
      "550 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.004334]\n",
      "Generating interpolations...\n",
      "551 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.008189]\n",
      "552 [D loss: 0.000259, acc.: 100.00%] [G loss: 0.016322]\n",
      "553 [D loss: 0.000209, acc.: 100.00%] [G loss: 0.010670]\n",
      "554 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.007238]\n",
      "555 [D loss: 0.000541, acc.: 100.00%] [G loss: 0.008834]\n",
      "556 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.005572]\n",
      "557 [D loss: 0.000221, acc.: 100.00%] [G loss: 0.013316]\n",
      "558 [D loss: 0.000108, acc.: 100.00%] [G loss: 0.005926]\n",
      "559 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.008206]\n",
      "560 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.012445]\n",
      "561 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.008279]\n",
      "562 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.018194]\n",
      "563 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.009036]\n",
      "564 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.009400]\n",
      "565 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.012467]\n",
      "566 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.004988]\n",
      "567 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.005313]\n",
      "568 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.009746]\n",
      "569 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.005699]\n",
      "570 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.008442]\n",
      "571 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.044336]\n",
      "572 [D loss: 0.000085, acc.: 100.00%] [G loss: 0.004922]\n",
      "573 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.014650]\n",
      "574 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.015153]\n",
      "575 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.011590]\n",
      "576 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.009734]\n",
      "577 [D loss: 0.001891, acc.: 100.00%] [G loss: 0.006293]\n",
      "578 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.012546]\n",
      "579 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.019775]\n",
      "580 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.004526]\n",
      "581 [D loss: 0.000183, acc.: 100.00%] [G loss: 0.007837]\n",
      "582 [D loss: 0.000208, acc.: 100.00%] [G loss: 0.007655]\n",
      "583 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.007942]\n",
      "584 [D loss: 0.000532, acc.: 100.00%] [G loss: 0.004979]\n",
      "585 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.004652]\n",
      "586 [D loss: 0.000262, acc.: 100.00%] [G loss: 0.012994]\n",
      "587 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.006394]\n",
      "588 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.013358]\n",
      "589 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.003143]\n",
      "590 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.005939]\n",
      "591 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.007490]\n",
      "592 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.004016]\n",
      "593 [D loss: 0.000290, acc.: 100.00%] [G loss: 0.004800]\n",
      "594 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.006286]\n",
      "595 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.006476]\n",
      "596 [D loss: 0.000082, acc.: 100.00%] [G loss: 0.005871]\n",
      "597 [D loss: 0.000124, acc.: 100.00%] [G loss: 0.003615]\n",
      "598 [D loss: 0.000123, acc.: 100.00%] [G loss: 0.005846]\n",
      "599 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.004354]\n",
      "600 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.004163]\n",
      "Generating interpolations...\n",
      "601 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.007960]\n",
      "602 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.012944]\n",
      "603 [D loss: 0.000101, acc.: 100.00%] [G loss: 0.003785]\n",
      "604 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.004378]\n",
      "605 [D loss: 0.000086, acc.: 100.00%] [G loss: 0.004446]\n",
      "606 [D loss: 0.000257, acc.: 100.00%] [G loss: 0.004613]\n",
      "607 [D loss: 0.000314, acc.: 100.00%] [G loss: 0.005788]\n",
      "608 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.007480]\n",
      "609 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.012815]\n",
      "610 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.005052]\n",
      "611 [D loss: 0.001472, acc.: 100.00%] [G loss: 0.009462]\n",
      "612 [D loss: 0.000078, acc.: 100.00%] [G loss: 0.006865]\n",
      "613 [D loss: 0.000447, acc.: 100.00%] [G loss: 0.003246]\n",
      "614 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.005404]\n",
      "615 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.001852]\n",
      "616 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.002926]\n",
      "617 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.049234]\n",
      "618 [D loss: 0.000255, acc.: 100.00%] [G loss: 0.004484]\n",
      "619 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.003833]\n",
      "620 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.003212]\n",
      "621 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.002989]\n",
      "622 [D loss: 0.000263, acc.: 100.00%] [G loss: 0.006214]\n",
      "623 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.017864]\n",
      "624 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.007319]\n",
      "625 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.004189]\n",
      "626 [D loss: 0.000916, acc.: 100.00%] [G loss: 0.005447]\n",
      "627 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.006466]\n",
      "628 [D loss: 0.000176, acc.: 100.00%] [G loss: 0.020488]\n",
      "629 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.007173]\n",
      "630 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.009809]\n",
      "631 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.009306]\n",
      "632 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.004657]\n",
      "633 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.014162]\n",
      "634 [D loss: 0.000087, acc.: 100.00%] [G loss: 0.004899]\n",
      "635 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.005342]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "636 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.001971]\n",
      "637 [D loss: 0.000110, acc.: 100.00%] [G loss: 0.002198]\n",
      "638 [D loss: 0.000351, acc.: 100.00%] [G loss: 0.008999]\n",
      "639 [D loss: 0.000228, acc.: 100.00%] [G loss: 0.006320]\n",
      "640 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.007282]\n",
      "641 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.008530]\n",
      "642 [D loss: 0.000091, acc.: 100.00%] [G loss: 0.003861]\n",
      "643 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.005135]\n",
      "644 [D loss: 0.000068, acc.: 100.00%] [G loss: 0.005642]\n",
      "645 [D loss: 0.000126, acc.: 100.00%] [G loss: 0.013575]\n",
      "646 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.005654]\n",
      "647 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.006136]\n",
      "648 [D loss: 0.000104, acc.: 100.00%] [G loss: 0.017105]\n",
      "649 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.011238]\n",
      "650 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.010027]\n",
      "Generating interpolations...\n",
      "651 [D loss: 0.000140, acc.: 100.00%] [G loss: 0.012990]\n",
      "652 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.006859]\n",
      "653 [D loss: 0.000109, acc.: 100.00%] [G loss: 0.003868]\n",
      "654 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.005824]\n",
      "655 [D loss: 0.000098, acc.: 100.00%] [G loss: 0.003762]\n",
      "656 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.007496]\n",
      "657 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.002939]\n",
      "658 [D loss: 0.000070, acc.: 100.00%] [G loss: 0.004366]\n",
      "659 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.003115]\n",
      "660 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.008174]\n",
      "661 [D loss: 0.002474, acc.: 100.00%] [G loss: 0.004938]\n",
      "662 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.004087]\n",
      "663 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.003733]\n",
      "664 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.008028]\n",
      "665 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.006047]\n",
      "666 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.002719]\n",
      "667 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.004722]\n",
      "668 [D loss: 0.000269, acc.: 100.00%] [G loss: 0.007506]\n",
      "669 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.007128]\n",
      "670 [D loss: 0.000231, acc.: 100.00%] [G loss: 0.003212]\n",
      "671 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.002646]\n",
      "672 [D loss: 0.000121, acc.: 100.00%] [G loss: 0.003312]\n",
      "673 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.002309]\n",
      "674 [D loss: 0.000437, acc.: 100.00%] [G loss: 0.005499]\n",
      "675 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.004092]\n",
      "676 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.003192]\n",
      "677 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.002456]\n",
      "678 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.006224]\n",
      "679 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.005382]\n",
      "680 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.008061]\n",
      "681 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.005383]\n",
      "682 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.005866]\n",
      "683 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.016322]\n",
      "684 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.011262]\n",
      "685 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.006563]\n",
      "686 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.002916]\n",
      "687 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.002365]\n",
      "688 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.006860]\n",
      "689 [D loss: 0.000504, acc.: 100.00%] [G loss: 0.006073]\n",
      "690 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.013551]\n",
      "691 [D loss: 0.000119, acc.: 100.00%] [G loss: 0.003306]\n",
      "692 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.003232]\n",
      "693 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.008162]\n",
      "694 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.002524]\n",
      "695 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.003627]\n",
      "696 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.002777]\n",
      "697 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.004330]\n",
      "698 [D loss: 0.000241, acc.: 100.00%] [G loss: 0.004722]\n",
      "699 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.006194]\n",
      "700 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.003920]\n",
      "Generating interpolations...\n",
      "701 [D loss: 0.000046, acc.: 100.00%] [G loss: 0.002812]\n",
      "702 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.006509]\n",
      "703 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.024521]\n",
      "704 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.008834]\n",
      "705 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.018896]\n",
      "706 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.002849]\n",
      "707 [D loss: 0.000073, acc.: 100.00%] [G loss: 0.004948]\n",
      "708 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.002032]\n",
      "709 [D loss: 0.000056, acc.: 100.00%] [G loss: 0.004962]\n",
      "710 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.008910]\n",
      "711 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.003644]\n",
      "712 [D loss: 0.000125, acc.: 100.00%] [G loss: 0.009912]\n",
      "713 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.006001]\n",
      "714 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.003133]\n",
      "715 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.005834]\n",
      "716 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.004406]\n",
      "717 [D loss: 0.000094, acc.: 100.00%] [G loss: 0.004274]\n",
      "718 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.023276]\n",
      "719 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.004903]\n",
      "720 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.004072]\n",
      "721 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.004629]\n",
      "722 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.006801]\n",
      "723 [D loss: 0.000341, acc.: 100.00%] [G loss: 0.015950]\n",
      "724 [D loss: 0.000063, acc.: 100.00%] [G loss: 0.009183]\n",
      "725 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.007362]\n",
      "726 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.011905]\n",
      "727 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.004797]\n",
      "728 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.003647]\n",
      "729 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.002482]\n",
      "730 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.006123]\n",
      "731 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.003606]\n",
      "732 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.007456]\n",
      "733 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.004719]\n",
      "734 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.006115]\n",
      "735 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.003503]\n",
      "736 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.011032]\n",
      "737 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.010958]\n",
      "738 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.005162]\n",
      "739 [D loss: 0.000112, acc.: 100.00%] [G loss: 0.020934]\n",
      "740 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.012723]\n",
      "741 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.007772]\n",
      "742 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.007296]\n",
      "743 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.003354]\n",
      "744 [D loss: 0.000115, acc.: 100.00%] [G loss: 0.007149]\n",
      "745 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.006753]\n",
      "746 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.005621]\n",
      "747 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.010226]\n",
      "748 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.011404]\n",
      "749 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.004243]\n",
      "750 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.006795]\n",
      "Generating interpolations...\n",
      "751 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.004192]\n",
      "752 [D loss: 0.000165, acc.: 100.00%] [G loss: 0.011031]\n",
      "753 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.007753]\n",
      "754 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.003466]\n",
      "755 [D loss: 0.000113, acc.: 100.00%] [G loss: 0.007541]\n",
      "756 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.006562]\n",
      "757 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.007923]\n",
      "758 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.004604]\n",
      "759 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.010117]\n",
      "760 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.009195]\n",
      "761 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.006567]\n",
      "762 [D loss: 0.000150, acc.: 100.00%] [G loss: 0.005360]\n",
      "763 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.007036]\n",
      "764 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.006256]\n",
      "765 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.002825]\n",
      "766 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.004035]\n",
      "767 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.009109]\n",
      "768 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.008773]\n",
      "769 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.004140]\n",
      "770 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.003382]\n",
      "771 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.006906]\n",
      "772 [D loss: 0.000099, acc.: 100.00%] [G loss: 0.006789]\n",
      "773 [D loss: 0.000074, acc.: 100.00%] [G loss: 0.005814]\n",
      "774 [D loss: 0.000144, acc.: 100.00%] [G loss: 0.004836]\n",
      "775 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.004651]\n",
      "776 [D loss: 0.000141, acc.: 100.00%] [G loss: 0.006796]\n",
      "777 [D loss: 0.000080, acc.: 100.00%] [G loss: 0.007354]\n",
      "778 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.005870]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "779 [D loss: 0.000057, acc.: 100.00%] [G loss: 0.005312]\n",
      "780 [D loss: 0.000406, acc.: 100.00%] [G loss: 0.015357]\n",
      "781 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.007568]\n",
      "782 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.006563]\n",
      "783 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.010393]\n",
      "784 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.009012]\n",
      "785 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.004913]\n",
      "786 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.007018]\n",
      "787 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.005495]\n",
      "788 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.001459]\n",
      "789 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.004276]\n",
      "790 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.007789]\n",
      "791 [D loss: 0.000171, acc.: 100.00%] [G loss: 0.004156]\n",
      "792 [D loss: 0.000006, acc.: 100.00%] [G loss: 0.004664]\n",
      "793 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.005124]\n",
      "794 [D loss: 0.000039, acc.: 100.00%] [G loss: 0.006694]\n",
      "795 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.004346]\n",
      "796 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.019339]\n",
      "797 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.005516]\n",
      "798 [D loss: 0.000052, acc.: 100.00%] [G loss: 0.006058]\n",
      "799 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.005136]\n",
      "800 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.004106]\n",
      "Generating interpolations...\n",
      "801 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.007245]\n",
      "802 [D loss: 0.000147, acc.: 100.00%] [G loss: 0.005921]\n",
      "803 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.003870]\n",
      "804 [D loss: 0.000088, acc.: 100.00%] [G loss: 0.004617]\n",
      "805 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.009313]\n",
      "806 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.004408]\n",
      "807 [D loss: 0.000036, acc.: 100.00%] [G loss: 0.006706]\n",
      "808 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.004100]\n",
      "809 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.006328]\n",
      "810 [D loss: 0.000132, acc.: 100.00%] [G loss: 0.005965]\n",
      "811 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.004239]\n",
      "812 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.006261]\n",
      "813 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.003706]\n",
      "814 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.010768]\n",
      "815 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.017119]\n",
      "816 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.002855]\n",
      "817 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.011592]\n",
      "818 [D loss: 0.000100, acc.: 100.00%] [G loss: 0.007003]\n",
      "819 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.005620]\n",
      "820 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.015626]\n",
      "821 [D loss: 0.000043, acc.: 100.00%] [G loss: 0.004379]\n",
      "822 [D loss: 0.000050, acc.: 100.00%] [G loss: 0.004325]\n",
      "823 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.007519]\n",
      "824 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.005597]\n",
      "825 [D loss: 0.000162, acc.: 100.00%] [G loss: 0.002790]\n",
      "826 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.006712]\n",
      "827 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.013165]\n",
      "828 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.006075]\n",
      "829 [D loss: 0.000097, acc.: 100.00%] [G loss: 0.006255]\n",
      "830 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.006118]\n",
      "831 [D loss: 0.000090, acc.: 100.00%] [G loss: 0.004385]\n",
      "832 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.006975]\n",
      "833 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.007401]\n",
      "834 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.001185]\n",
      "835 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.006439]\n",
      "836 [D loss: 0.000081, acc.: 100.00%] [G loss: 0.002607]\n",
      "837 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.007767]\n",
      "838 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.002756]\n",
      "839 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.003284]\n",
      "840 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.003075]\n",
      "841 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.007919]\n",
      "842 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.017317]\n",
      "843 [D loss: 0.000072, acc.: 100.00%] [G loss: 0.020496]\n",
      "844 [D loss: 0.000095, acc.: 100.00%] [G loss: 0.004692]\n",
      "845 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.006109]\n",
      "846 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.003071]\n",
      "847 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.010441]\n",
      "848 [D loss: 0.000064, acc.: 100.00%] [G loss: 0.006900]\n",
      "849 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.006402]\n",
      "850 [D loss: 0.000065, acc.: 100.00%] [G loss: 0.004317]\n",
      "Generating interpolations...\n",
      "851 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.005817]\n",
      "852 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.005256]\n",
      "853 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.005390]\n",
      "854 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.003080]\n",
      "855 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.009006]\n",
      "856 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.014150]\n",
      "857 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.007648]\n",
      "858 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.007158]\n",
      "859 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.004797]\n",
      "860 [D loss: 0.000037, acc.: 100.00%] [G loss: 0.008373]\n",
      "861 [D loss: 0.000027, acc.: 100.00%] [G loss: 0.004750]\n",
      "862 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.006056]\n",
      "863 [D loss: 0.000092, acc.: 100.00%] [G loss: 0.005341]\n",
      "864 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.007394]\n",
      "865 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.004839]\n",
      "866 [D loss: 0.000060, acc.: 100.00%] [G loss: 0.011615]\n",
      "867 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.003077]\n",
      "868 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.011639]\n",
      "869 [D loss: 0.000102, acc.: 100.00%] [G loss: 0.005644]\n",
      "870 [D loss: 0.000053, acc.: 100.00%] [G loss: 0.008722]\n",
      "871 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.008695]\n",
      "872 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.003015]\n",
      "873 [D loss: 0.000011, acc.: 100.00%] [G loss: 0.007790]\n",
      "874 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.004085]\n",
      "875 [D loss: 0.001012, acc.: 100.00%] [G loss: 0.002950]\n",
      "876 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.004435]\n",
      "877 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.001831]\n",
      "878 [D loss: 0.000145, acc.: 100.00%] [G loss: 0.005555]\n",
      "879 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.004383]\n",
      "880 [D loss: 0.000061, acc.: 100.00%] [G loss: 0.007097]\n",
      "881 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.004252]\n",
      "882 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.003331]\n",
      "883 [D loss: 0.000044, acc.: 100.00%] [G loss: 0.010865]\n",
      "884 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.005541]\n",
      "885 [D loss: 0.000022, acc.: 100.00%] [G loss: 0.007930]\n",
      "886 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.004637]\n",
      "887 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.002931]\n",
      "888 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.006630]\n",
      "889 [D loss: 0.000058, acc.: 100.00%] [G loss: 0.003452]\n",
      "890 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.002524]\n",
      "891 [D loss: 0.001030, acc.: 100.00%] [G loss: 0.005184]\n",
      "892 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.002299]\n",
      "893 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.004231]\n",
      "894 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.002612]\n",
      "895 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.002627]\n",
      "896 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.004227]\n",
      "897 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.006283]\n",
      "898 [D loss: 0.000041, acc.: 100.00%] [G loss: 0.004815]\n",
      "899 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.003420]\n",
      "900 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.006258]\n",
      "Generating interpolations...\n",
      "901 [D loss: 0.000079, acc.: 100.00%] [G loss: 0.004750]\n",
      "902 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.011129]\n",
      "903 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.004788]\n",
      "904 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.005137]\n",
      "905 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.002243]\n",
      "906 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.002240]\n",
      "907 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.004128]\n",
      "908 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.003696]\n",
      "909 [D loss: 0.000069, acc.: 100.00%] [G loss: 0.005107]\n",
      "910 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.007100]\n",
      "911 [D loss: 0.000029, acc.: 100.00%] [G loss: 0.002673]\n",
      "912 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.002168]\n",
      "913 [D loss: 0.000048, acc.: 100.00%] [G loss: 0.004602]\n",
      "914 [D loss: 0.000300, acc.: 100.00%] [G loss: 0.003827]\n",
      "915 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.006994]\n",
      "916 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.006565]\n",
      "917 [D loss: 0.000018, acc.: 100.00%] [G loss: 0.003773]\n",
      "918 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.003260]\n",
      "919 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.003197]\n",
      "920 [D loss: 0.000051, acc.: 100.00%] [G loss: 0.002801]\n",
      "921 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.005879]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "922 [D loss: 0.000111, acc.: 100.00%] [G loss: 0.004447]\n",
      "923 [D loss: 0.000005, acc.: 100.00%] [G loss: 0.006520]\n",
      "924 [D loss: 0.000133, acc.: 100.00%] [G loss: 0.006886]\n",
      "925 [D loss: 0.000025, acc.: 100.00%] [G loss: 0.001367]\n",
      "926 [D loss: 0.000574, acc.: 100.00%] [G loss: 0.004090]\n",
      "927 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.008319]\n",
      "928 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.002718]\n",
      "929 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.004509]\n",
      "930 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.005441]\n",
      "931 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.003493]\n",
      "932 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.002659]\n",
      "933 [D loss: 0.000059, acc.: 100.00%] [G loss: 0.002755]\n",
      "934 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.035641]\n",
      "935 [D loss: 0.000083, acc.: 100.00%] [G loss: 0.002792]\n",
      "936 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.008220]\n",
      "937 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.001520]\n",
      "938 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.002085]\n",
      "939 [D loss: 0.000031, acc.: 100.00%] [G loss: 0.002612]\n",
      "940 [D loss: 0.000049, acc.: 100.00%] [G loss: 0.003301]\n",
      "941 [D loss: 0.000054, acc.: 100.00%] [G loss: 0.001412]\n",
      "942 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.002515]\n",
      "943 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.003418]\n",
      "944 [D loss: 0.000040, acc.: 100.00%] [G loss: 0.002858]\n",
      "945 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.005172]\n",
      "946 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.002085]\n",
      "947 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.002745]\n",
      "948 [D loss: 0.000034, acc.: 100.00%] [G loss: 0.002489]\n",
      "949 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.004757]\n",
      "950 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.006267]\n",
      "Generating interpolations...\n",
      "951 [D loss: 0.000032, acc.: 100.00%] [G loss: 0.009296]\n",
      "952 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.005242]\n",
      "953 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.003580]\n",
      "954 [D loss: 0.000128, acc.: 100.00%] [G loss: 0.008432]\n",
      "955 [D loss: 0.000023, acc.: 100.00%] [G loss: 0.002824]\n",
      "956 [D loss: 0.000021, acc.: 100.00%] [G loss: 0.004681]\n",
      "957 [D loss: 0.000038, acc.: 100.00%] [G loss: 0.015483]\n",
      "958 [D loss: 0.000045, acc.: 100.00%] [G loss: 0.010917]\n",
      "959 [D loss: 0.000007, acc.: 100.00%] [G loss: 0.005944]\n",
      "960 [D loss: 0.000033, acc.: 100.00%] [G loss: 0.001863]\n",
      "961 [D loss: 0.000071, acc.: 100.00%] [G loss: 0.001775]\n",
      "962 [D loss: 0.000042, acc.: 100.00%] [G loss: 0.004516]\n",
      "963 [D loss: 0.000093, acc.: 100.00%] [G loss: 0.005821]\n",
      "964 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.002767]\n",
      "965 [D loss: 0.000077, acc.: 100.00%] [G loss: 0.003567]\n",
      "966 [D loss: 0.000016, acc.: 100.00%] [G loss: 0.004177]\n",
      "967 [D loss: 0.000020, acc.: 100.00%] [G loss: 0.013214]\n",
      "968 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.004221]\n",
      "969 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.013409]\n",
      "970 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.007845]\n",
      "971 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.009809]\n",
      "972 [D loss: 0.000028, acc.: 100.00%] [G loss: 0.003050]\n",
      "973 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.002873]\n",
      "974 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.005015]\n",
      "975 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.002028]\n",
      "976 [D loss: 0.000024, acc.: 100.00%] [G loss: 0.013492]\n",
      "977 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.004444]\n",
      "978 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.004488]\n",
      "979 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.001389]\n",
      "980 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.003739]\n",
      "981 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.003842]\n",
      "982 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.009032]\n",
      "983 [D loss: 0.000030, acc.: 100.00%] [G loss: 0.004772]\n",
      "984 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.007457]\n",
      "985 [D loss: 0.000191, acc.: 100.00%] [G loss: 0.006997]\n",
      "986 [D loss: 0.000013, acc.: 100.00%] [G loss: 0.007641]\n",
      "987 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.006417]\n",
      "988 [D loss: 0.000015, acc.: 100.00%] [G loss: 0.005328]\n",
      "989 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.004758]\n",
      "990 [D loss: 0.000047, acc.: 100.00%] [G loss: 0.005901]\n",
      "991 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.009763]\n",
      "992 [D loss: 0.000017, acc.: 100.00%] [G loss: 0.006788]\n",
      "993 [D loss: 0.000106, acc.: 100.00%] [G loss: 0.007761]\n",
      "994 [D loss: 0.000008, acc.: 100.00%] [G loss: 0.005151]\n",
      "995 [D loss: 0.000019, acc.: 100.00%] [G loss: 0.007215]\n",
      "996 [D loss: 0.000067, acc.: 100.00%] [G loss: 0.004878]\n",
      "997 [D loss: 0.000012, acc.: 100.00%] [G loss: 0.010308]\n",
      "998 [D loss: 0.000066, acc.: 100.00%] [G loss: 0.012675]\n",
      "999 [D loss: 0.000009, acc.: 100.00%] [G loss: 0.004364]\n",
      "1000 [D loss: 0.000035, acc.: 100.00%] [G loss: 0.005723]\n",
      "Generating interpolations...\n",
      "1001 [D loss: 0.000055, acc.: 100.00%] [G loss: 0.005022]\n",
      "1002 [D loss: 0.000076, acc.: 100.00%] [G loss: 0.006489]\n",
      "1003 [D loss: 0.000026, acc.: 100.00%] [G loss: 0.005949]\n",
      "1004 [D loss: 0.000010, acc.: 100.00%] [G loss: 0.000960]\n",
      "1005 [D loss: 0.000014, acc.: 100.00%] [G loss: 0.005287]\n",
      "1006 [D loss: 0.000084, acc.: 100.00%] [G loss: 0.003155]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-3365f2b2e5c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mcheck_noise\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcheck_noise\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m         \u001b[0mc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m     )\n",
      "\u001b[0;32m<ipython-input-3-c7eb98ddffb2>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, iterations, batch_size, save_interval, model_interval, check_noise, r, c)\u001b[0m\n\u001b[1;32m    118\u001b[0m             \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mz_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0mg_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcombined\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%d [D loss: %f, acc.: %.2f%%] [G loss: %f]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0md_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1346\u001b[0m                                                     class_weight)\n\u001b[1;32m   1347\u001b[0m       \u001b[0mtrain_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_train_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1348\u001b[0;31m       \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1350\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    578\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    579\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 580\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    581\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    609\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    610\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 611\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    612\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    613\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2418\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2419\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2420\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2422\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1663\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1664\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1665\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1666\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1667\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1744\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1745\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1746\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1747\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1748\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    596\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    597\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 598\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    599\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    600\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0;32m---> 60\u001b[0;31m                                         inputs, attrs, num_outputs)\n\u001b[0m\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    dcgan = DCGAN()\n",
    "    r, c = 5, 5\n",
    "    check_noise = np.random.uniform(-1, 1, (r * c, 100))\n",
    "    dcgan.train(\n",
    "        iterations=200000,\n",
    "        batch_size=32,\n",
    "        # save_interval=1000,\n",
    "        save_interval=50, ### epoch回数が50の倍数になったときに、generator生成画像を保存\n",
    "        model_interval=5000,\n",
    "        check_noise=check_noise,\n",
    "        r=r,\n",
    "        c=c\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
